{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 -  Text Classification Task\n",
    "Developed by Group 05:\n",
    "- Emanuel Maia - up202107486\n",
    "- Rita Leite - up202105309\n",
    "- Tiago Azevedo - up202108699"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Introduction\n",
    "\n",
    "This project is a continuation of the first one, but now we are exploring the use of Hugging Face Transformers. That said, the theme of the project and the structure of our data remain the same, consisting of publications sourced from Reddit and Google, authored by individuals from England, Australia, and India.\n",
    "\n",
    "The Reddit-sourced data is divided as follows:\n",
    "- Reddit (England): Training data and test data\n",
    "- Reddit (Australia): Training data and test data\n",
    "- Reddit (India): Training data and test data\n",
    "\n",
    "Similarly, an equivalent division applies to the Google-sourced data:\n",
    "- Google (England): Training data and test data\n",
    "- Google (Australia): Training data and test data\n",
    "- Google (India): Training data and test data\n",
    "\n",
    "All datasets share the same attributes: `id`, a unique identifier for each entry, `text`, the content of the publication, and `sentiment_label`, the target variable for our analysis. The `sentiment_label` is binary, where 0 indicates a negative sentiment and 1 indicates a positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Preparation\n",
    "\n",
    "The process begins by importing the necessary libraries and implementing utility functions that immediately handle tasks such as language detection and translation. These early steps ensure that all input text is brought to a common language, English, making it suitable for consistent processing and analysis across multilingual data.\n",
    "\n",
    "Data from Reddit and Google, covering user content from the UK, India, and Australia, is first loaded and merged by region. To ensure consistency, text entries from India are automatically translated into English. All datasets are then standardized by aligning their structure and renaming relevant columns. Finally, the regional datasets are combined into a single unified collection, ready for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import contractions\n",
    "import evaluate\n",
    "import torch\n",
    "import nltk\n",
    "import re\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, AutoModelForSequenceClassification, RobertaForSequenceClassification, TrainingArguments, Trainer, DataCollatorWithPadding, pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "from datasets import Dataset, DatasetDict\n",
    "from googletrans import Translator\n",
    "from langdetect import detect\n",
    "\n",
    "translator = Translator()\n",
    "def translate_text(text):\n",
    "    if detect(text) == \"en\":\n",
    "        return text\n",
    "    else:\n",
    "        try:\n",
    "            return translator.translate(text, src=\"hi\", dest=\"en\").text\n",
    "        except:\n",
    "            try:\n",
    "                return translator.translate(text, src=\"ur\", dest=\"en\").text\n",
    "            except:\n",
    "                try:\n",
    "                    return translator.translate(text, src=\"bn\", dest=\"en\").text\n",
    "                except:\n",
    "                    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read Reddit-sourced data \n",
    "reddit_uk_train = pd.read_json(\"data/reddit-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_train = pd.read_json(\"data/reddit-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_train = pd.read_json(\"data/reddit-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_uk_valid = pd.read_json(\"data/reddit-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_valid = pd.read_json(\"data/reddit-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_valid = pd.read_json(\"data/reddit-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# Read Google-sourced data \n",
    "google_uk_train = pd.read_json(\"data/google-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_train = pd.read_json(\"data/google-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_train = pd.read_json(\"data/google-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_uk_valid = pd.read_json(\"data/google-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_valid = pd.read_json(\"data/google-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_valid = pd.read_json(\"data/google-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# Merge and translate data by country\n",
    "uk_union = pd.concat([reddit_uk_train, reddit_uk_valid, google_uk_train, google_uk_valid], ignore_index=True)\n",
    "au_union = pd.concat([reddit_au_train, reddit_au_valid, google_au_train, google_au_valid], ignore_index=True)\n",
    "in_union = pd.concat([reddit_in_train, reddit_in_valid, google_in_train, google_in_valid], ignore_index=True)\n",
    "in_union[\"text\"] = in_union[\"text\"].apply(translate_text)\n",
    "\n",
    "# Rename columns for huggingface datasets\n",
    "uk_union.rename(columns={'sentiment_label':'label'}, inplace = True)\n",
    "au_union.rename(columns={'sentiment_label':'label'}, inplace = True)\n",
    "in_union.rename(columns={'sentiment_label':'label'}, inplace = True)\n",
    "\n",
    "# Merge all data\n",
    "gl_union = pd.concat([uk_union, au_union, in_union]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Preprocessing\n",
    "\n",
    "The process begins by downloading necessary resources from the NLTK library, such as stopwords, tokenizers, and lemmatizers. A set of stopwords is loaded, with some additional words specifically removed to prevent them from being filtered out during preprocessing. Following that, a lemmatization function is applied, which processes the text by identifying the parts of speech and reducing the words to their base forms accordingly.\n",
    "\n",
    "Next, a text preprocessing procedure is performed on the datasets, which includes expanding contractions (e.g., \"I'm\" to \"I am\"), cleaning non-ASCII characters and unnecessary symbols, converting all text to lowercase for consistency, and removing any extra spaces. The text is then tokenized into words, stopwords are eliminated to focus on more meaningful terms, and lemmatization is applied to reduce words to their root forms.\n",
    "\n",
    "This comprehensive preprocessing ensures that the datasets from the UK, Australia, India, and a combined global set are standardized and ready for analysis, improving the quality and consistency of the text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Prepare stopwords to not include the ones with negation value\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words_remove = {\"no\", \"not\", \"nor\", \"t\"}\n",
    "stop_words.difference_update(stop_words_remove)\n",
    "\n",
    "# Apply lemmatization (using pos tagging)\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "def lemmatize_with_pos(text):\n",
    "    words = token(text)   \n",
    "    words_tag = nltk.pos_tag(words)   \n",
    "    words_lem = []\n",
    "    for word, tag in words_tag:\n",
    "        if tag.startswith('N'): \n",
    "            words_lem.append(lemma.lemmatize(word, pos='n'))\n",
    "        elif tag.startswith('V'): \n",
    "            words_lem.append(lemma.lemmatize(word, pos='v'))\n",
    "        elif tag.startswith('J'): \n",
    "            words_lem.append(lemma.lemmatize(word, pos='a'))\n",
    "        elif tag.startswith('R'): \n",
    "            words_lem.append(lemma.lemmatize(word, pos='r'))\n",
    "        else: \n",
    "            words_lem.append(lemma.lemmatize(word))\n",
    "    return words_lem\n",
    "\n",
    "# Apply preprocessing techniques\n",
    "token = nltk.word_tokenize\n",
    "def text_pre_processing(dataset):\n",
    "    dataset['text'] = dataset['text'].apply(contractions.fix)\n",
    "    dataset['text'] = dataset[\"text\"].apply(lambda x: re.sub(r'[^\\x00-\\x7F]|[^a-zA-Z ]', ' ', x).strip())\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(str.lower)\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "    dataset['text'] = dataset['text'].apply(token)    \n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: lemmatize_with_pos(\" \".join(x)))\n",
    "    dataset['text'] = [\" \".join(text) for text in dataset[\"text\"]]\n",
    "    return dataset  \n",
    "       \n",
    "uk_union = text_pre_processing(uk_union)\n",
    "au_union = text_pre_processing(au_union)\n",
    "in_union = text_pre_processing(in_union)\n",
    "gl_union = text_pre_processing(gl_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 - Transformers\n",
    "\n",
    "In this section, we define several functions necessary for preprocessing the text data, partitioning it into training, validation, and test sets, and evaluating the model's performance during training.\n",
    "\n",
    "We also define the models that we will evaluate, that are the following:\n",
    "\n",
    "- **DistilBERT Base Uncased fine-tuned on SST-2** ( learn more clicking [here](https://dataloop.ai/library/model/distilbert_distilbert-base-uncased-finetuned-sst-2-english/) )\n",
    "\n",
    "    - Overview\n",
    "        - Distilled version of BERT, meaning it has been compressed to be smaller and faster while retaining most of BERT’s performance.\n",
    "        - It has been specifically fine-tuned on the Stanford Sentiment Treebank (SST-2) dataset, which contains sentences labeled with either positive or negative sentiment.\n",
    "        - Well-suited for binary sentiment classification tasks. \n",
    "\n",
    "    - Strengths\n",
    "        - High accuracy, as it achieves around 91.3% on the SST-2 development set.\n",
    "        - Its compact size makes it more efficient to use, particularly in environments with limited computational resources.\n",
    "    \n",
    "    - Weaknesses\n",
    "        - It may exhibit biased behavior, especially toward underrepresented populations, as a result of biases in its training data.\n",
    "        - It is not designed for factual reasoning or objective classification beyond the specific scope of sentiment analysis.\n",
    "    \n",
    "    - Reason\n",
    "        - We selected this model because it offers a good balance of performance and efficiency. As a distilled version of BERT, it retains much of the accuracy while being smaller and faster, making it ideal for text classification tasks. Specifically fine-tuned on the SST-2 dataset for sentiment analysis, it is well-suited for binary sentiment classification, which aligns perfectly with our project’s goals.\n",
    "\n",
    "- **TabularisAI Multilingual Sentiment Analysis** ( learn more clicking [here](https://dataloop.ai/library/model/tabularisai_multilingual-sentiment-analysis/) )\n",
    "    \n",
    "    - Overview\n",
    "        - A multilingual sentiment analysis model designed to handle text in multiple languages.\n",
    "        - Trained to classify sentiment into five distinct categories: very negative, negative, neutral, positive, and very positive.\n",
    "    \n",
    "    - Strengths\n",
    "        - Multilingual support enables sentiment analysis without the need for prior translation, useful in international and multicultural contexts.\n",
    "\n",
    "    - Weaknesses\n",
    "        - The five-level classification may be overly complex for tasks that require only a binary sentiment distinction (e.g., positive vs. negative).\n",
    "        - Performance can vary depending on the language and input quality; lower-resourced languages may have reduced accuracy.\n",
    "        - Multilingual models typically require more computational resources than single-language, task-specific models.\n",
    "\n",
    "    - Reason\n",
    "        - We selected this model because of its ability to handle multiple languages, including Hindi and Bengali. While we translate texts that are not entirely in English during preprocessing, some texts cannot be fully translated. In these cases, we believe this model’s multilingual capabilities will allow it to better handle such texts and provide more accurate sentiment analysis across different languages.\n",
    "\n",
    "- **CardiffNLP Twitter RoBERTa Base Sentiment Latest** ( learn more clicking [here](https://dataloop.ai/library/model/cardiffnlp_twitter-roberta-base-sentiment-latest/) )\n",
    "\n",
    "    - Overview\n",
    "        - A RoBERTa-based model fine-tuned specifically on a large collection of tweets for sentiment analysis.\n",
    "        - It classifies sentiment into three categories: negative, neutral, and positive.\n",
    "        - Designed to handle informal, short-form text typical of social media platforms like Twitter.\n",
    "\n",
    "    - Strengths\n",
    "        - Optimized for real-world, noisy text data such as tweets, making it robust to slang, abbreviations, and emojis.\n",
    "        - Good performance in social media contexts where traditional models may struggle.\n",
    "        - Fine-tuned using manually labeled tweets, improving accuracy in sentiment prediction for tweet-style text.\n",
    "\n",
    "    - Weaknesses\n",
    "        - May underperform when applied to formal or long-form text outside the social media domain.\n",
    "    \n",
    "    - Reason\n",
    "        - We selected this model because it is specifically designed for analyzing sentiment in informal texts, like tweets. Since part of our dataset contains texts with similar characteristics, including abbreviations and less formal language, we believe this model is well-suited to handle these cases more effectively than models trained on formal datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cut the text\n",
    "def apply_text_cut(text):\n",
    "    return ' '.join(text.split()[:100])\n",
    "\n",
    "# Divide the text in train, validation and test\n",
    "def apply_partition(sample):\n",
    "    sample_hf = Dataset.from_pandas(sample)\n",
    "    train_test = sample_hf.train_test_split(test_size=0.2)\n",
    "    valid_test = train_test['test'].train_test_split(test_size=0.5)\n",
    "    return DatasetDict({\n",
    "        'train': train_test['train'],\n",
    "        'validation': valid_test['train'],\n",
    "        'test': valid_test['test']\n",
    "    })\n",
    "\n",
    "# Summarize the text\n",
    "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\")\n",
    "def apply_summarizer(text):\n",
    "    try:\n",
    "        summary = summarizer(\n",
    "            ' '.join(text.split()[:512]),\n",
    "            max_length=100,\n",
    "            min_length=10,\n",
    "            do_sample=False\n",
    "            )[0][\"summary_text\"]\n",
    "        return summary\n",
    "    except Exception as e:\n",
    "        print(f\"Summarization failed: {e}\")\n",
    "        return text\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 - Without Fine-Tunning\n",
    "\n",
    "We evaluate the performance of pretrained language models on our own sentiment classification dataset, without additional fine-tuning. The goal is to assess how well these models, trained on general-purpose sentiment data, generalize to our specific domain.\n",
    "\n",
    "The steps involved are:\n",
    "- Load the pretrained model and tokenizer using the Hugging Face Transformers library;\n",
    "- Preprocess the dataset (text truncation, tokenization, padding);\n",
    "- Use the model to predict sentiment labels on our test set;\n",
    "- Evaluate performance using standard metrics: accuracy, precision, recall, F1-score, and the confusion matrix.\n",
    "\n",
    "To accelerate the classification process and due to our computational limitations, we chose to limit textual features to a maximum of 100 tokens. To achieve this, we followed two different approaches: the first involved truncating the text to the first 100 words, and the second used a text summarization model to condense longer texts into a maximum of 100 tokens.\n",
    "\n",
    "As can be seen from the results, there was little difference in terms of precision or accuracy between the two methods. However, as expected, the simple truncation approach proved to be more advantageous, as it is significantly faster than the summarization method.\n",
    "\n",
    "This serves as a baseline for comparison before applying more advanced techniques like fine-tuning or parameter-efficient adaptation (e.g., LoRA)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models used without training\n",
    "models_pretrain = [\n",
    "    {'name': \"distilbert-base-uncased-finetuned-sst-2-english\", \"type\": AutoModelForSequenceClassification, \"label\": \"label\", \"num\": 2},\n",
    "    {'name': \"tabularisai/multilingual-sentiment-analysis\", \"type\": AutoModelForSequenceClassification, \"label\": \"label\", \"num\": 5},\n",
    "    {'name': \"cardiffnlp/twitter-roberta-base-sentiment-latest\", \"type\": RobertaForSequenceClassification, \"label\": \"Sentiment\", \"num\": 3}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_pretrain(model_dict, sample, use_summary=False):\n",
    "    \n",
    "    if(model_dict[\"label\"] != \"label\"):\n",
    "        sample = sample.rename(columns={'label':'Sentiment'})\n",
    "    \n",
    "    # initialize the model and tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dict[\"name\"])\n",
    "    model = model_dict[\"type\"].from_pretrained(model_dict[\"name\"])\n",
    "    trainer = Trainer(model=model, compute_metrics=compute_metrics)  \n",
    "    \n",
    "    # prepare the data\n",
    "    if use_summary:\n",
    "        sample[\"text\"] = sample[\"text\"].apply(lambda t: apply_summarizer(t) if len(t.split()) > 100 else t)\n",
    "    sample['text'] = sample['text'].apply(apply_text_cut)\n",
    "          \n",
    "    MAX_LENGTH = 128  \n",
    "    sample_hf = apply_partition(sample)\n",
    "    sample_hf = sample_hf.map(lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH), batched=True)\n",
    "    \n",
    "    # predict the labels\n",
    "    predictions = trainer.predict(test_dataset=sample_hf[\"test\"])\n",
    "    y_pred = np.argmax(predictions.predictions, axis=-1)\n",
    "    y_test = sample_hf[\"test\"][model_dict[\"label\"]]\n",
    "    \n",
    "    if model_dict[\"num\"] == 3:\n",
    "        y_pred = [1 if pred in [1, 2] else 0 for pred in y_pred]\n",
    "    elif model_dict[\"num\"] == 5:\n",
    "        y_pred = [1 if pred in [2, 3, 4] else 0 for pred in y_pred]\n",
    "    \n",
    "    # print confusion matrix and metrics\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print('Accuracy:', round(accuracy_score(y_test, y_pred),2))\n",
    "    print('Precision:', round(precision_score(y_test, y_pred, average='macro'),2))\n",
    "    print('Recall:', round(recall_score(y_test, y_pred, average='macro'),2))\n",
    "    print('F1:', round(f1_score(y_test, y_pred, average='macro'),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_pretrain:\n",
    "    print(\"Results for model:\", model[\"name\"])\n",
    "    apply_model_pretrain(model, gl_union.copy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_pretrain:\n",
    "    print(\"Results for model:\", model[\"name\"])\n",
    "    apply_model_pretrain(model, gl_union.copy(), True)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 - With Fine-Tunning\n",
    "\n",
    "Next, we fine-tuned some models using our dataset to improve classification performance. For this, we trained it for 3 epochs.\n",
    "\n",
    "As you can observe, the results achieved by the same model improved significantly after being fine-tuned on our own dataset. This highlights the importance of task-specific training, as the model was able to better adapt to the characteristics and nuances of our data, ultimately leading to higher classification performance compared to its pre-trained-only usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Models used with traning\n",
    "models_tunning = [\n",
    "    {'name': \"distilbert-base-uncased-finetuned-sst-2-english\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_tunning(model_name, sample, target_modules, use_lora = False):\n",
    "    \n",
    "    # initialize the model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_name)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "    \n",
    "    # prepare the data\n",
    "    MAX_LENGTH = 128\n",
    "    sample['text'] = sample['text'].apply(apply_text_cut)\n",
    "    sample_hf = apply_partition(sample)\n",
    "    sample_hf = sample_hf.map(\n",
    "        lambda x: tokenizer(x[\"text\"], truncation=True, padding=\"max_length\", max_length=MAX_LENGTH), \n",
    "        batched=True\n",
    "    )\n",
    "\n",
    "    if (use_lora):\n",
    "        lora_config = LoraConfig(\n",
    "            task_type=TaskType.SEQ_CLS,\n",
    "            r=8,\n",
    "            lora_alpha=16,\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            target_modules=target_modules,\n",
    "        )\n",
    "        model = get_peft_model(model, lora_config)\n",
    "        \n",
    "    # set the training arguments\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        learning_rate=2e-5,\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        num_train_epochs=3,\n",
    "        weight_decay=0.01,\n",
    "        eval_strategy=\"epoch\",\n",
    "        # save_strategy=\"epoch\",\n",
    "        save_strategy=\"no\",\n",
    "        load_best_model_at_end=False,\n",
    "    )\n",
    "\n",
    "    # initialize the trainer \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=sample_hf[\"train\"],\n",
    "        eval_dataset=sample_hf[\"validation\"],\n",
    "        tokenizer=tokenizer,\n",
    "        data_collator=data_collator,\n",
    "        compute_metrics=compute_metrics\n",
    "    )\n",
    "\n",
    "    # train the model\n",
    "    trainer.train()\n",
    "    eval_results = trainer.evaluate()\n",
    "    predictions = trainer.predict(test_dataset=sample_hf[\"test\"])\n",
    "    print(\"Evaluation results:\", eval_results)\n",
    "    print(\"Predictions:\", predictions.predictions)\n",
    "    print(\"Predicted labels:\", np.argmax(predictions.predictions, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_tunning:\n",
    "    print(\"Results for model:\", model[\"name\"])\n",
    "    apply_model_tunning(model[\"name\"], gl_union.copy(), [\"q_lin\", \"k_lin\", \"v_lin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 - With Prompting\n",
    "\n",
    "We also explored the use of prompting for our classification task by leveraging the \"google/flan-t5-small\" model. This model is a fine-tuned version of Google's T5 architecture, adapted through instruction tuning on a wide range of tasks. Although it is relatively lightweight, it is capable of understanding and responding to task-specific prompts in natural language.\n",
    "\n",
    "In our setup, we prompted the model to classify input text as either positive or negative, aligning with the goal of our task. We also included a note in the prompt highlighting that the input text could potentially be in Hindi, Urdu, or Bengali. This was an important consideration, as misclassification of texts in these languages was a major limitation encountered by our best-performing model in the first project.\n",
    "\n",
    "However, although it produced better results than the other models we tested without fine-tuning, the binary text classification model we trained on our dataset still achieved superior performance overall."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_prompting = [\n",
    "    {'name': \"google/flan-t5-small\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_model_prompting(model_name, sample, text_column=\"text\", label_column=\"label\"):\n",
    "    sample['text'] = sample['text'].apply(apply_text_cut)\n",
    "    sample_hf = apply_partition(sample)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    pipe = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "    def classify_batch_prompts(texts, batch_size=8):\n",
    "        prompts = [\n",
    "            f\"Classify the following text as positive or negative: '{text}'. Pay attention because the text, although in principle in English, may also be or have parts in Hindi, Urdu or Bengali. Try to be as accurate as you can.\"\n",
    "            for text in texts\n",
    "        ]\n",
    "\n",
    "        results = pipe(prompts, max_new_tokens=4, batch_size=batch_size, truncation=True)\n",
    "        labels = []\n",
    "\n",
    "        for res in results:\n",
    "            response = res['generated_text'].strip().lower()\n",
    "            if \"positive\" in response:\n",
    "                labels.append(1)\n",
    "            elif \"negative\" in response:\n",
    "                labels.append(0)\n",
    "            else:\n",
    "                labels.append(0)\n",
    "\n",
    "        return labels\n",
    "\n",
    "    texts = sample_hf[\"test\"][text_column]\n",
    "    true_labels = sample_hf[\"test\"][label_column]\n",
    "    pred_labels = classify_batch_prompts(texts)\n",
    "\n",
    "    acc = accuracy_score(true_labels, pred_labels)\n",
    "    prec = precision_score(true_labels, pred_labels)\n",
    "    rec = recall_score(true_labels, pred_labels)\n",
    "    f1 = f1_score(true_labels, pred_labels)\n",
    "\n",
    "    print(f\"Accuracy: {acc:.2f}\")\n",
    "    print(f\"Precision: {prec:.2f}\")\n",
    "    print(f\"Recall: {rec:.2f}\")\n",
    "    print(f\"F1 Score: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model in models_prompting:\n",
    "    apply_model_prompting(model[\"name\"], gl_union.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5 - Error Analysis\n",
    "\n",
    "We will now analyze the errors made by our best-performing model. As previously observed, our best model was the DistilBERT Base Uncased fine-tuned on SST-2, which achieved an accuracy of 0.86 after being fine-tuned on our dataset for 3 epochs.\n",
    "\n",
    "As we did in the previous project, we will now analyze the model’s performance when trained on subsets of the dataset it was originally fine-tuned on. Specifically, we divide our dataset based on the country of origin of each sample, creating three separate training sets: one for Australia, one for the United Kingdom, and one for India.\n",
    "\n",
    "We adopt this approach because, in the previous project, it allowed us to identify one of the main weaknesses of our best-performing model: its poor classification performance on the subset containing only data from India."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_tunning(models_tunning[0][\"name\"], au_union.copy(), [\"q_lin\", \"k_lin\", \"v_lin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_tunning(models_tunning[0][\"name\"], uk_union.copy(), [\"q_lin\", \"k_lin\", \"v_lin\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "apply_model_tunning(models_tunning[0][\"name\"], in_union.copy(), [\"q_lin\", \"k_lin\", \"v_lin\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As can be observed, even after fine-tuning the model on country-specific datasets, it is evident that the model struggles to adapt effectively to the dataset from India. This may be due to the use of region-specific expressions that are uncommon in standard English, or due to portions of the text being written in other languages such as Hindi, Urdu, or Bengali."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
