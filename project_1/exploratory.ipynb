{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 -  Text Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Provenance and Characteristics\n",
    "\n",
    "The dataset we will be working with consists of publications sourced from Reddit and Google, authored by individuals from England, Australia, and India. \n",
    "\n",
    "The Reddit-sourced data is divided as follows:\n",
    "\n",
    "- Reddit (England): Training data and test data\n",
    "- Reddit (Australia): Training data and test data\n",
    "- Reddit (India): Training data and test data\n",
    "\n",
    "Similarly, an equivalent division applies to the Google-sourced data:\n",
    "\n",
    "- Google (England): Training data and test data\n",
    "- Google (Australia): Training data and test data\n",
    "- Google (India): Training data and test data\n",
    "\n",
    "All datasets share the same attributes: `id`, a unique identifier for each entry, `text`, the content of the publication, and `sentiment_label`, the target variable for our analysis. The `sentiment_label` is binary, where `0` indicates a negative sentiment and `1` indicates a positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Initial Setup\n",
    "\n",
    "We begin by reading all 12 datasets. Since the distinction between training and test data is not relevant for our analysis, we first merge them, reducing the total to 6 datasets.\n",
    "\n",
    "To further facilitate analysis, we also create 3 additional datasets, grouping the data by country of origin. In this step, we combine Reddit and Google data while keeping separate datasets for England, Australia, and India Moreover, we also create 3 datasets, grouping the data by their source, this is Reddit or Google.\n",
    "\n",
    "In the end, we also created a global dataset, that is, with all the data we have available.\n",
    "\n",
    "In addition, we remove the `id` attribute at the beginning of our process. This decision was made to prevent inconsistencies, as some publications shared the same `id` across different datasets. Keeping this attribute could lead to ambiguity in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Reddit-sourced data \n",
    "reddit_uk_train = pd.read_json(\"data/reddit-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_train = pd.read_json(\"data/reddit-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_train = pd.read_json(\"data/reddit-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_uk_valid = pd.read_json(\"data/reddit-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_valid = pd.read_json(\"data/reddit-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_valid = pd.read_json(\"data/reddit-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# read Google-sourced data \n",
    "google_uk_train = pd.read_json(\"data/google-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_train = pd.read_json(\"data/google-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_train = pd.read_json(\"data/google-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_uk_valid = pd.read_json(\"data/google-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_valid = pd.read_json(\"data/google-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_valid = pd.read_json(\"data/google-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# merge Reddit-sourced data by country\n",
    "reddit_uk_union = pd.concat([reddit_uk_train, reddit_uk_valid], ignore_index=True)\n",
    "reddit_au_union = pd.concat([reddit_au_train, reddit_au_valid], ignore_index=True)\n",
    "reddit_in_union = pd.concat([reddit_in_train, reddit_in_valid], ignore_index=True)\n",
    "\n",
    "# merge Google-sourced data by country\n",
    "google_uk_union = pd.concat([google_uk_train, google_uk_valid], ignore_index=True)\n",
    "google_au_union = pd.concat([google_au_train, google_au_valid], ignore_index=True)\n",
    "google_in_union = pd.concat([google_in_train, google_in_valid], ignore_index=True)\n",
    "\n",
    "# merge data by country\n",
    "uk_union = pd.concat([reddit_uk_union, google_uk_union], ignore_index=True)\n",
    "au_union = pd.concat([reddit_au_union, google_au_union], ignore_index=True)\n",
    "in_union = pd.concat([reddit_in_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge data by source\n",
    "reddit_union = pd.concat([reddit_uk_union, reddit_au_union, reddit_in_union], ignore_index=True)\n",
    "google_union = pd.concat([google_uk_union, google_au_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge all data\n",
    "global_union = pd.concat([reddit_union, google_union])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Class Distribution by Source\n",
    "\n",
    "We begin by comparing the number of entries from Reddit and Google. Our analysis shows that both sources contain approximately the same number of entries.\n",
    "\n",
    "Next, we analyzed the distribution of the sentiment class in both sources. This analysis reveals that Reddit data is predominantly negative, while Google data is mostly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From Reddit there are \" + str(len(reddit_union)))\n",
    "print(\"From Google there are \" + str(len(google_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by source\n",
    "reddit_counts = reddit_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "google_counts = google_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "reddit_counts.columns = [\"sentiment_label\", \"Reddit\"]\n",
    "google_counts.columns = [\"sentiment_label\", \"Google\"]\n",
    "df = pd.merge(reddit_counts, google_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df.melt(id_vars=\"sentiment_label\", var_name=\"Source\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Source\", palette=[\"#1f77b4\", \"#ff7f0e\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of Sentiment Label Distribution: Reddit vs Google\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Source\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Class Distribution by Country\n",
    "\n",
    "Similarly to the source analysis, we also examined the distribution of the data across countries, as well as the balance of the target class distribution.\n",
    "\n",
    "As shown below, the datasets have roughly the same number of entries, and the target class is approximately evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From UK there are \" + str(len(uk_union)))\n",
    "print(\"From AU there are \" + str(len(au_union)))\n",
    "print(\"From IN there are \" + str(len(in_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by country\n",
    "uk_counts = uk_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "au_counts = au_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "in_counts = in_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "uk_counts.columns = [\"sentiment_label\", \"UK\"]\n",
    "au_counts.columns = [\"sentiment_label\", \"AU\"]\n",
    "in_counts.columns = [\"sentiment_label\", \"IN\"]\n",
    "df_counts = pd.merge(uk_counts, au_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_counts = pd.merge(df_counts, in_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df_counts.melt(id_vars=\"sentiment_label\", var_name=\"Country\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Country\", palette=[\"#1f77b4\", \"#ff7f0e\", \"#2f8c1f\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of Sentiment Label Distribution: UK vs AU vs IN\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Word Distribution by Source\n",
    "\n",
    "Next, we analyze the distribution of words by source, focusing on the top 10 words. As shown, despite the words appearing in different positions across the sources, there are 3 common words. This low number may be due to the topic of the posts, which can be very different between platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate and plot TF-IDF\n",
    "def plot_tfidf(dataset, title):\n",
    "    \n",
    "    # get text column\n",
    "    texts = dataset[\"text\"]  \n",
    "    \n",
    "    # start TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=20)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)   \n",
    "    terms = vectorizer.get_feature_names_out()   \n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)   \n",
    "    sum_tfidf = tfidf_df.sum(axis=0)   \n",
    "    sorted_tfidf = sum_tfidf.sort_values(ascending=False)\n",
    "    \n",
    "    # plot graph\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sorted_tfidf.head(10).plot(kind='bar')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('TF-IDF Score')\n",
    "    plt.xlabel('Words')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    return sorted_tfidf.head(10).index.tolist()\n",
    "\n",
    "def plot_wordcolud(dataset):\n",
    "    wordcloud = WordCloud().generate(\" \".join(dataset[\"text\"].dropna().astype(str)))\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_top = plot_tfidf(reddit_union, 'Top 10 TF-IDF Words for Reddit Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(reddit_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_top = plot_tfidf(google_union, 'Top 10 TF-IDF Words for Google Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(google_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_top = list(set(reddit_top) & set(google_top))\n",
    "\n",
    "print(\"There are \" + str(len(comumn_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Word Distribution by Country\n",
    "\n",
    "We also analyzed the distribution of words by country. As shown, the vocabulary does not seem to vary significantly across countries, as the number of common words between two datasets ranges from 7 to 8 out of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_top = plot_tfidf(uk_union, 'Top 10 TF-IDF Words for UK Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(uk_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_top = plot_tfidf(au_union, 'Top 10 TF-IDF Words for AU Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(au_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_top = plot_tfidf(in_union, 'Top 10 TF-IDF Words for IN Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(in_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_uk_au_top = list(set(uk_top) & set(au_top))\n",
    "comumn_au_in_top = list(set(au_top) & set(in_top))\n",
    "comumn_in_uk_top = list(set(in_top) & set(uk_top))\n",
    "\n",
    "print(\"UK and AU\")\n",
    "print(\"There are \" + str(len(comumn_uk_au_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_uk_au_top))\n",
    "\n",
    "print(\"\\nAU and IN\")\n",
    "print(\"There are \" + str(len(comumn_au_in_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_au_in_top))\n",
    "\n",
    "print(\"\\nIN and UK\")\n",
    "print(\"There are \" + str(len(comumn_in_uk_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_in_uk_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Word Distribuition by Class\n",
    "\n",
    "We also analyzed the word distribution by class, separating the publications based on their connotation, positive or negative.\n",
    "\n",
    "In positive publications, words like \"good,\" \"nice,\" and \"great\" stand out, which aligns with their typical use in expressing positive sentiments.\n",
    "\n",
    "Conversely, in negative publications, these words are noticeably less common. While \"good\" still appears frequently, it is not as prominent as in positive posts. Additionally, the word \"n't\" is highly recurrent, highlighting its role in negation and negative sentiment formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(global_union[global_union[\"sentiment_label\"] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(global_union[global_union[\"sentiment_label\"] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.word_tokenize\n",
    "stemmer = nltk.PorterStemmer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "stop_words.remove(\"no\")\n",
    "stop_words.remove(\"not\")\n",
    "stop_words.remove(\"nor\")\n",
    "stop_words.remove(\"t\")\n",
    "\n",
    "# remove words related with negation\n",
    "stop_words_remove = {\"no\", \"not\", \"nor\", \"t\"}\n",
    "stop_words.difference_update(stop_words_remove)\n",
    "\n",
    "def text_clean(dataset):\n",
    "    dataset['text'] = dataset[\"text\"].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x).strip())\n",
    "    # convert to lowercase\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(str.lower)\n",
    "    # remove multiple spaces\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip()) \n",
    "    \n",
    "\n",
    "def text_parser(dataset):  \n",
    "    # with tokenizer and stemmer\n",
    "    dataset['text_tokst'] = dataset['text'].apply(tokenizer)\n",
    "    dataset['text_tokst'] = dataset['text_tokst'].apply(lambda x: [stemmer.stem(w) for w in x])\n",
    "    # with spacy (separates didn't into do and not)\n",
    "    dataset['text_spacy'] = dataset['text'].apply(lambda x : [w.lemma_ for w in nlp(x)])\n",
    "    # remove stopwords\n",
    "    dataset['text_tokst'] = dataset['text_tokst'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    dataset['text_spacy'] = dataset['text_spacy'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "\n",
    "\n",
    "text_clean(uk_union)\n",
    "text_clean(au_union)\n",
    "text_clean(in_union)\n",
    "\n",
    "text_parser(uk_union)\n",
    "text_parser(au_union)\n",
    "text_parser(in_union)\n",
    "\n",
    "print(uk_union['text_tokst'].iloc[0])\n",
    "print(uk_union['text_spacy'].iloc[0])\n",
    "\n",
    "print(len(uk_union['text_tokst'].iloc[0]))\n",
    "print(len(uk_union['text_spacy'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data_prepared\"\n",
    "\n",
    "# Criar a pasta se ela n√£o existir\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "# Salvar os DataFrames como CSV (ou outro formato)\n",
    "uk_union.to_csv(os.path.join(folder_path, \"uk_union.csv\"), index=False)\n",
    "au_union.to_csv(os.path.join(folder_path, \"au_union.csv\"), index=False)\n",
    "in_union.to_csv(os.path.join(folder_path, \"in_union.csv\"), index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
