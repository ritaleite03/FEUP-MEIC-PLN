{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 -  Text Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Provenance and Characteristics\n",
    "\n",
    "The dataset we will be working with consists of publications sourced from Reddit and Google, authored by individuals from England, Australia, and India. \n",
    "\n",
    "The Reddit-sourced data is divided as follows:\n",
    "\n",
    "- Reddit (England): Training data and test data\n",
    "- Reddit (Australia): Training data and test data\n",
    "- Reddit (India): Training data and test data\n",
    "\n",
    "Similarly, an equivalent division applies to the Google-sourced data:\n",
    "\n",
    "- Google (England): Training data and test data\n",
    "- Google (Australia): Training data and test data\n",
    "- Google (India): Training data and test data\n",
    "\n",
    "All datasets share the same attributes: `id`, a unique identifier for each entry, `text`, the content of the publication, and `sentiment_label`, the target variable for our analysis. The `sentiment_label` is binary, where `0` indicates a negative sentiment and `1` indicates a positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Initial Setup\n",
    "\n",
    "We begin by reading all 12 datasets. Since the distinction between training and test data is not relevant for our analysis, we first merge them, reducing the total to 6 datasets.\n",
    "\n",
    "To further facilitate analysis, we also create 3 additional datasets, grouping the data by country of origin. In this step, we combine Reddit and Google data while keeping separate datasets for England, Australia, and India Moreover, we also create 3 datasets, grouping the data by their source, this is Reddit or Google.\n",
    "\n",
    "In the end, we also created a global dataset, that is, with all the data we have available.\n",
    "\n",
    "In addition, we remove the `id` attribute at the beginning of our process. This decision was made to prevent inconsistencies, as some publications shared the same `id` across different datasets. Keeping this attribute could lead to ambiguity in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import gensim\n",
    "import numpy as np\n",
    "import math\n",
    "import contractions\n",
    "import string\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Reddit-sourced data \n",
    "reddit_uk_train = pd.read_json(\"data/reddit-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_train = pd.read_json(\"data/reddit-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_train = pd.read_json(\"data/reddit-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_uk_valid = pd.read_json(\"data/reddit-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_valid = pd.read_json(\"data/reddit-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_valid = pd.read_json(\"data/reddit-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# read Google-sourced data \n",
    "google_uk_train = pd.read_json(\"data/google-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_train = pd.read_json(\"data/google-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_train = pd.read_json(\"data/google-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_uk_valid = pd.read_json(\"data/google-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_valid = pd.read_json(\"data/google-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_valid = pd.read_json(\"data/google-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# merge Reddit-sourced data by country\n",
    "reddit_uk_union = pd.concat([reddit_uk_train, reddit_uk_valid], ignore_index=True)\n",
    "reddit_au_union = pd.concat([reddit_au_train, reddit_au_valid], ignore_index=True)\n",
    "reddit_in_union = pd.concat([reddit_in_train, reddit_in_valid], ignore_index=True)\n",
    "\n",
    "# merge Google-sourced data by country\n",
    "google_uk_union = pd.concat([google_uk_train, google_uk_valid], ignore_index=True)\n",
    "google_au_union = pd.concat([google_au_train, google_au_valid], ignore_index=True)\n",
    "google_in_union = pd.concat([google_in_train, google_in_valid], ignore_index=True)\n",
    "\n",
    "# merge data by country\n",
    "uk_union = pd.concat([reddit_uk_union, google_uk_union], ignore_index=True)\n",
    "au_union = pd.concat([reddit_au_union, google_au_union], ignore_index=True)\n",
    "in_union = pd.concat([reddit_in_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge data by source\n",
    "reddit_union = pd.concat([reddit_uk_union, reddit_au_union, reddit_in_union], ignore_index=True)\n",
    "google_union = pd.concat([google_uk_union, google_au_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge all data\n",
    "global_union = pd.concat([reddit_union, google_union])\n",
    "test_global = global_union.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(values, labels, title=\"Pie Chart\"):\n",
    "    plt.figure(figsize=(2 * len(values), len(values)))\n",
    "    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)   \n",
    "    plt.title(title)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie_chart([len(global_union[global_union[\"sentiment_label\"] == 0]), len(global_union[global_union[\"sentiment_label\"] == 1])], [\"Negative\", \"Positive\"], title=\"Entries by Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Class Distribution by Source\n",
    "\n",
    "We begin by comparing the number of entries from Reddit and Google. Our analysis shows that both sources contain approximately the same number of entries.\n",
    "\n",
    "Next, we analyzed the distribution of the sentiment class in both sources. This analysis reveals that Reddit data is predominantly negative, while Google data is mostly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From Reddit there are \" + str(len(reddit_union)))\n",
    "print(\"From Google there are \" + str(len(google_union)))\n",
    "plot_pie_chart([len(reddit_union), len(google_union)], [\"Reddit\", \"Google\"], title=\"Entries by Source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by source\n",
    "reddit_counts = reddit_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "google_counts = google_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "reddit_counts.columns = [\"sentiment_label\", \"Reddit\"]\n",
    "google_counts.columns = [\"sentiment_label\", \"Google\"]\n",
    "df = pd.merge(reddit_counts, google_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df.melt(id_vars=\"sentiment_label\", var_name=\"Source\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Source\", palette=[\"#1f77b4\", \"#ff7f0e\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of target label by source: Reddit vs Google\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Source\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Class Distribution by Country\n",
    "\n",
    "Similarly to the source analysis, we also examined the distribution of the data across countries, as well as the balance of the target class distribution.\n",
    "\n",
    "As shown below, the datasets have roughly the same number of entries, and the target class is approximately evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From UK there are \" + str(len(uk_union)))\n",
    "print(\"From AU there are \" + str(len(au_union)))\n",
    "print(\"From IN there are \" + str(len(in_union)))\n",
    "plot_pie_chart([len(uk_union), len(au_union), len(in_union)], [\"UK\", \"AU\", \"IN\"], title=\"Entries by Country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by country\n",
    "uk_counts = uk_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "au_counts = au_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "in_counts = in_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "uk_counts.columns = [\"sentiment_label\", \"UK\"]\n",
    "au_counts.columns = [\"sentiment_label\", \"AU\"]\n",
    "in_counts.columns = [\"sentiment_label\", \"IN\"]\n",
    "df_counts = pd.merge(uk_counts, au_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_counts = pd.merge(df_counts, in_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df_counts.melt(id_vars=\"sentiment_label\", var_name=\"Country\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Country\", palette=[\"#1f77b4\", \"#ff7f0e\", \"#2f8c1f\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of target label by country: UK vs AU vs IN\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Word Distribution by Source\n",
    "\n",
    "Next, we analyze the distribution of words by source, focusing on the top 10 words. As shown, despite the words appearing in different positions across the sources, there are 3 common words. This low number may be due to the topic of the posts, which can be very different between platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate and plot TF-IDF\n",
    "def plot_tfidf(dataset, title):\n",
    "    \n",
    "    # get text column\n",
    "    texts = dataset[\"text\"]  \n",
    "    \n",
    "    # start TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)   \n",
    "    terms = vectorizer.get_feature_names_out()   \n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)   \n",
    "    sum_tfidf = tfidf_df.sum(axis=0)   \n",
    "    sorted_tfidf = sum_tfidf.sort_values(ascending=False)\n",
    "    \n",
    "    # plot graph\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(sorted_tfidf)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return sorted_tfidf.head(10).index.tolist()\n",
    "\n",
    "def plot_wordcolud(dataset):\n",
    "    wordcloud = WordCloud().generate(\" \".join(dataset[\"text\"].dropna().astype(str)))\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_top = plot_tfidf(reddit_union, 'Top 100 TF-IDF Words for Reddit Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_top = plot_tfidf(google_union, 'Top 100 TF-IDF Words for Google Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_top = list(set(reddit_top) & set(google_top))\n",
    "\n",
    "print(\"There are \" + str(len(comumn_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Word Distribution by Country\n",
    "\n",
    "We also analyzed the distribution of words by country. As shown, the vocabulary does not seem to vary significantly across countries, as the number of common words between two datasets ranges from 7 to 8 out of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_top = plot_tfidf(uk_union, 'Top 100 TF-IDF Words for UK Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_top = plot_tfidf(au_union, 'Top 100 TF-IDF Words for AU Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_top = plot_tfidf(in_union, 'Top 100 TF-IDF Words for IN Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_uk_au_top = list(set(uk_top) & set(au_top))\n",
    "comumn_au_in_top = list(set(au_top) & set(in_top))\n",
    "comumn_in_uk_top = list(set(in_top) & set(uk_top))\n",
    "\n",
    "print(\"UK and AU\")\n",
    "print(\"There are \" + str(len(comumn_uk_au_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_uk_au_top))\n",
    "\n",
    "print(\"\\nAU and IN\")\n",
    "print(\"There are \" + str(len(comumn_au_in_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_au_in_top))\n",
    "\n",
    "print(\"\\nIN and UK\")\n",
    "print(\"There are \" + str(len(comumn_in_uk_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_in_uk_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Pre-Processing\n",
    "\n",
    "In this step, we remove all characters that are not alphabetical or whitespace. Additionally, we convert the text to lowercase and eliminate consecutive spaces. After cleaning the text, we perform tokenization and lemmazation. Finally, we remove words from the stopwords list, except for those with negation, as they are crucial for our classification task.\n",
    "\n",
    "We initially experimented with stemming as an alternative to lemmatization but ultimately chose the latter. At first, our lemmatization approach did not account for a word’s context, leading to incorrect results. Upon further investigation, we discovered the importance of specifying the \"pos\" parameter in the \"lemmatize\" function, which determines whether a word is a verb, noun, adjective, or adverb. To ensure accurate classification, we leveraged the \"pos_tag\" function from NLTK to assign the appropriate part of speech before lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words_remove = {\"no\", \"not\", \"nor\", \"t\"}\n",
    "stop_words.difference_update(stop_words_remove)\n",
    "\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "token = nltk.word_tokenize\n",
    "\n",
    "def lemmatize_with_pos(text):\n",
    "    words = token(text)   \n",
    "    words_tag = nltk.pos_tag(words)   \n",
    "    words_lem = []\n",
    "    for word, tag in words_tag:\n",
    "        if tag.startswith('N'): words_lem.append(lemma.lemmatize(word, pos='n')) # noun\n",
    "        elif tag.startswith('V'): words_lem.append(lemma.lemmatize(word, pos='v')) # verb\n",
    "        elif tag.startswith('J'): words_lem.append(lemma.lemmatize(word, pos='a')) # adjective\n",
    "        elif tag.startswith('R'): words_lem.append(lemma.lemmatize(word, pos='r')) # adverb\n",
    "        else: words_lem.append(lemma.lemmatize(word))\n",
    "    return words_lem\n",
    "\n",
    "def text_pre_processing(dataset):\n",
    "    # text_vader\n",
    "    dataset['text_vader'] = dataset['text'].apply(contractions.fix)\n",
    "    dataset['text_vader'] = dataset[\"text_vader\"].apply(lambda x: re.sub(r'[^\\x00-\\x7F]|[^a-zA-Z ]', ' ', x).strip()) # remove all the caracteres that do not belong to the alphabet and are not a whitespace\n",
    "    dataset[\"text_vader\"] = dataset[\"text_vader\"].apply(str.lower) # converte all caracteres to lowercase\n",
    "    dataset[\"text_vader\"] = dataset[\"text_vader\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip()) # remove multiple whitespaces  \n",
    "    # text_processed\n",
    "    dataset['text_processed'] = dataset['text_vader'].apply(token) # apply tokenization    \n",
    "    dataset['text_processed'] = dataset['text_processed'].apply(lambda x: [word for word in x if word not in stop_words]) # remove stopwords, mantaining words like 'no', 'not', 'nor', 't'\n",
    "    # # # apply stemization\n",
    "    # # dataset['text_processed'] = dataset['text_processed'].apply(lambda x: [nltk.PorterStemmer().stem(w) for w in x])    \n",
    "    dataset['text_processed'] = dataset['text_processed'].apply(lambda x: lemmatize_with_pos(\" \".join(x))) # apply lemmazation   \n",
    "    dataset['text_processed'] = [\" \".join(text) for text in dataset[\"text_processed\"]] # join words from the list in a sentence    \n",
    "    return dataset\n",
    "       \n",
    "def metrics(y_test, y_pred, time):\n",
    "    [[tp, fp], [fn, tn]] = (confusion_matrix(y_test, y_pred))\n",
    "    print(\"                    predicted positive   predicted negative\")\n",
    "    print(\"real positive       \" + str(tp) + (\" \" * (len(str(fp)) - 1)) + \"                \" + str(fn))\n",
    "    print(\"real negative       \" + str(fp) + (\" \" * (len(str(tp)) - 1)) + \"                \" + str(tn))\n",
    "    print(f\"\\nAccuracy {accuracy_score(y_test, y_pred):.2f} Precision {precision_score(y_test, y_pred):.2f} Recall {recall_score(y_test, y_pred):.2f} F1 Score {f1_score(y_test, y_pred):.2f} Time {time:.2f}\") \n",
    "\n",
    "text_pre_processing(global_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making these modifications, we checked for words that were not recognized in NLTK's vocabulary, which likely indicated spelling errors. We attempted to use libraries like TextBlob to correct these mistakes but ultimately decided against it due to the high processing time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "\n",
    "valid_words = set(nltk.corpus.words.words())\n",
    "invalid_words = set()\n",
    "invalid_entries = set()\n",
    "\n",
    "corpus = global_union[\"text_processed\"].dropna().astype(str).tolist()\n",
    "for idx, comment in enumerate(corpus):\n",
    "    words = comment.split()\n",
    "    for word in words:\n",
    "        if word not in valid_words:\n",
    "            invalid_words.add(word)\n",
    "            invalid_entries.add(idx)\n",
    "\n",
    "print(\"There are \" + str(len(invalid_entries)) + \" entries with invalid words\")\n",
    "print(\"The words are \" + str(invalid_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then deleted the entries that had an empty \"text_processed\" attribute. This could happen due to pre-processing, for example, if the text attribute only consisted of characters that did not belong to the alphabet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before we had \" + str(len(global_union)) + \" entries.\")\n",
    "global_union = global_union[global_union[\"text_processed\"].str.len() > 0]\n",
    "print(\"Now we have \" + str(len(global_union)) + \" entries.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have made these modifications to the datasets, we save them to our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data_prepared\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "global_union.to_csv(os.path.join(folder_path, \"global_union.csv\"), index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Representation Technique\n",
    "Subsequently, we applied a range of feature representation techniques, exploring both sparse and dense vector approaches.\n",
    "\n",
    "In relation to sparse vectors:\n",
    "\n",
    "- **CountVectorizer**: the text is converted into a \"bag of words\" (BoW), where the order of the words is not considered. The focus is solely on how many times each word appears in the document, without taking into account the sequence or context in which it occurs.\n",
    "\n",
    "- **TfidfVectorizer**: is similar to CountVectorizer, but with an important difference: it adjusts the word counts by considering not only the frequency of a word in a specific text but also its relative importance in the larger set of documents.\n",
    "\n",
    "- **ngram_range in TfidfVectorizer**: the `ngram_range` parameter in TfidfVectorizer allows the model to capture n-grams. By setting `ngram_range` to `(1,2)`, the model will consider both unigrams (individual words) and bigrams (pairs of consecutive words), allowing it to capture more contextual information between nearby words.\n",
    "\n",
    "In relation to dense vectors:\n",
    "\n",
    "- **Word2Vec**: used to generate dense vector representations of words in a continuous vector space, where words with similar meanings are close together in the vector space.\n",
    "\n",
    "- **GloVe**: is also a word embedding technique, but unlike Word2Vec, which is based on a predictive model, GloVe is based on word co-occurrence matrices in a corpus. It tries to capture the global relationships between words, making it effective for representing word meanings in broader contexts.\n",
    "\n",
    "We also utilize **VADER**, implementing two different approaches. The first, a simpler approach, calculates the polarity of each word and classifies it as positive or negative. The second, a more complex approach, is an algorithm inspired by one from the professor's slides, which considers additional factors beyond polarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = global_union[\"text_processed\"].dropna().astype(str).tolist()\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "print(\"Text Processed's characteristic:\")\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Sparse Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Representation with sparse vectors:\\n\")\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "x_CountVectorizer = vectorizer.fit_transform(corpus)\n",
    "print(\"  - CountVectorizer's shape is \" + str(x_CountVectorizer.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None)\n",
    "x_TfidfVectorizer = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizer's shape is \" + str(x_TfidfVectorizer.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(1,2)) # unigram and bigram\n",
    "x_TfidfVectorizerUB = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizerUB's shape is\" + str(x_TfidfVectorizerUB.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(2,2)) # bigram\n",
    "x_TfidfVectorizerB = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizerB's shape is \" + str(x_TfidfVectorizerB.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Dense Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Representation with dense vectors:\\n\")\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len):\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError: True\n",
    "        finally: i += 1\n",
    "    \n",
    "    for j in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "        \n",
    "    return vec\n",
    "\n",
    "def text_to_vector_glove(embeddings, text, sequence_len, embedding_dim=100):\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:\n",
    "        if tokens[i] in embeddings:\n",
    "            vec.extend(embeddings[tokens[i]])\n",
    "            n += 1\n",
    "        i += 1\n",
    "   \n",
    "    for _ in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embedding_dim,))\n",
    "        \n",
    "    return vec\n",
    "\n",
    "temp_corpus = [x.split() for x in corpus]\n",
    "model = gensim.models.Word2Vec(temp_corpus, vector_size=100, window=3, min_count=2, workers=10, sg=1)\n",
    "embeddings_corpus = []\n",
    "for c in corpus:\n",
    "    embeddings_corpus.append(text_to_vector(model.wv, c, 10))\n",
    "\n",
    "x_Word2Vec = np.array(embeddings_corpus)\n",
    "print(\"  - Word2Vec's shape is \" + str(x_Word2Vec.shape))\n",
    "\n",
    "word_vectors = {}\n",
    "with open(\"data_prepared/glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        word_vectors[word] = vector\n",
    "\n",
    "embeddings_corpus_glove = [text_to_vector_glove(word_vectors, c, 10) for c in corpus]\n",
    "x_GloVe = np.array(embeddings_corpus_glove)\n",
    "print(\"  - GloVe's shape is \" + str(x_GloVe.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "y_pred = []\n",
    "\n",
    "start_time = time.time()\n",
    "for comment in global_union['text']:\n",
    "    y_pred.append(1 if sia.polarity_scores(comment)['compound'] > 0 else 0)\n",
    "end_time = time.time()\n",
    "\n",
    "metrics(global_union['sentiment_label'], y_pred, end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1_list, x2_list, x3_list, x4_list, x5_list, x6_list = [], [], [], [], [], []\n",
    "word_info = dict()\n",
    "pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\", \"you\", \"your\", \"yours\"}\n",
    "\n",
    "for line in global_union[\"text_vader\"]:\n",
    "    words = line.split()\n",
    "    x1, x2 = 0, 0\n",
    "    for word in words:\n",
    "        \n",
    "        # if word was already checked\n",
    "        if word in word_info:\n",
    "            if word_info[word] == 1: x1 += 1\n",
    "            elif word_info[word] == -1: x2 += 1\n",
    "            \n",
    "        # if not calculate polarity and add to the dictionary\n",
    "        else:\n",
    "            neg_polarity = sia.polarity_scores(word)[\"neg\"]\n",
    "            pos_polarity = sia.polarity_scores(word)[\"pos\"]\n",
    "            cpd_polarity = sia.polarity_scores(word)[\"compound\"]\n",
    "            if cpd_polarity > 0.05 and pos_polarity > neg_polarity:\n",
    "                word_info[word] = 1\n",
    "                x1 += 1\n",
    "            elif cpd_polarity < -0.05 and pos_polarity < neg_polarity:\n",
    "                word_info[word] = -1\n",
    "                x2 += 1\n",
    "            else:\n",
    "                word_info[word] = 0\n",
    "    \n",
    "    x1_list.append(x1)\n",
    "    x2_list.append(x2)\n",
    "    x3_list.append(1 if \"no\" in words else 0)\n",
    "    x4_list.append(sum(1 for word in words if word in pronouns))\n",
    "    x5_list.append(1 if \"!\" in words else 0)\n",
    "    x6_list.append(math.log(len(words) + 1))\n",
    "\n",
    "global_union[\"x1\"] = x1_list # number of positive words\n",
    "global_union[\"x2\"] = x2_list # number of negative words\n",
    "global_union[\"x3\"] = x3_list # 1 if \"no\" is present and 0 otherwise\n",
    "global_union[\"x4\"] = x4_list # number of pronouns\n",
    "global_union[\"x5\"] = x5_list # 1 if \"!\" is present and 0 otherwise\n",
    "global_union[\"x6\"] = x6_list # logarithm of text length for normalization.\n",
    "    \n",
    "x_vader = global_union[[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'LogisticRegression': {\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "        'max_iter': [100, 200, 500]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'loss': ['hinge', 'log'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "    }\n",
    "}\n",
    "\n",
    "def parameter_tuning(clf, x_predict, y_predict):\n",
    "    grid_search = GridSearchCV(clf, parameters[str(clf.__class__.__name__)], cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(x_predict, y_predict)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "    \n",
    "\n",
    "def predict_nb(x, y):\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    y_pred = clf.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "    return y_pred\n",
    "\n",
    "def predict_log(x, y):\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=42)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    # parameter tunning and training \n",
    "    model, param = parameter_tuning(clf, X_train, y_train)\n",
    "    start_time = time.time()   \n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    print(f\"\\nBets parameters: {param}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "    return y_pred\n",
    "    \n",
    "def predict_sgd(x, y):\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=42)\n",
    "    clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    # parameter tunning and training \n",
    "    model, param = parameter_tuning(clf, X_train, y_train)\n",
    "    start_time = time.time()   \n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    print(f\"\\nBets parameters: {param}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline\n",
    "\n",
    "Initially, we developed a baseline model where the text was not subjected to any preprocessing, and the feature representation was based on a Bag of Words approach. For classification, we utilized the Multinomial Naive Bayes (MultinomialNB) algorithm.\n",
    "\n",
    "Comparing the size of the feature space in our baseline model, which utilizes raw text without any preprocessing, we observe that it is significantly larger than the feature space of the preprocessed text. Specifically, the baseline model results in **28,874 features**, whereas the preprocessed text yields **16,551 features**, making the feature space approximately **1.7 times larger** in the absence of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None, lowercase=False, binary=True)\n",
    "x_baseline = vectorizer.fit_transform(global_union[\"text\"].dropna().astype(str).tolist())\n",
    "print(\"Baseline - \" + str(x_baseline.shape))\n",
    "predict_nb(x_baseline, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Multinomial Naive Bayes\n",
    "\n",
    "Comparing the results obtained with the preprocessed text to those of the baseline model, we observe that the differences in error rates are minimal. However, the models using preprocessed text demonstrate **substantially faster training times**.\n",
    "\n",
    "We can also observe that, regarding the use of n-grams, the model utilizing only bigrams yields the poorest results. In contrast, the model that combines unigrams and bigrams achieves better performance. This approach strikes a good balance, potentially improving the model's ability to handle cases involving negation more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse representations\n",
    "predict_nb(x_CountVectorizer, global_union['sentiment_label'])\n",
    "predict_nb(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "predict_nb(x_TfidfVectorizerUB, global_union['sentiment_label'])\n",
    "predict_nb(x_TfidfVectorizerB, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Logistic Regression\n",
    "\n",
    "Based on the results obtained previously, we decided to discontinue testing models with features extracted using TfidfVectorizer and bigrams. In addition, we explored feature representations using dense vectors, generated through two different methods: **Word2Vec** and **GloVe**.  \n",
    "\n",
    "When comparing Logistic Regression with Multinomial Naive Bayes, we observed that Logistic Regression consistently outperforms Naive Bayes in terms of accuracy. However, it requires a longer training time due to its iterative optimization process. In contrast, Naive Bayes is significantly faster, but its performance is generally inferior.  \n",
    "\n",
    "Additionally, we found that models trained with sparse vector representations tend to yield better results than those using dense vector representations. Notably, Word2Vec-based features performed significantly worse, which may be attributed to the limited size of our dataset and vocabulary. This likely prevented the Word2Vec model from effectively capturing the semantic relationships between words, resulting in poorer feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse representations\n",
    "predict_log(x_CountVectorizer, global_union['sentiment_label'])\n",
    "predict_log(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "predict_log(x_TfidfVectorizerUB, global_union['sentiment_label'])\n",
    "# dense representations\n",
    "predict_log(x_Word2Vec, global_union['sentiment_label'])\n",
    "predict_log(x_GloVe, global_union['sentiment_label'])\n",
    "# vader representation\n",
    "predict_log(x_vader, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Stochastic Gradient Descent (SGD) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse representations\n",
    "predict_sgd(x_CountVectorizer, global_union['sentiment_label'])\n",
    "predict_sgd(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "predict_sgd(x_TfidfVectorizerUB, global_union['sentiment_label'])\n",
    "# dense representations\n",
    "predict_sgd(x_Word2Vec, global_union['sentiment_label'])\n",
    "predict_sgd(x_GloVe, global_union['sentiment_label'])\n",
    "# vader representation\n",
    "predict_sgd(x_vader, global_union['sentiment_label'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
