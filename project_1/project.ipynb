{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 -  Text Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Provenance and Characteristics\n",
    "\n",
    "The dataset we will be working with consists of publications sourced from Reddit and Google, authored by individuals from England, Australia, and India. \n",
    "\n",
    "The Reddit-sourced data is divided as follows:\n",
    "\n",
    "- Reddit (England): Training data and test data\n",
    "- Reddit (Australia): Training data and test data\n",
    "- Reddit (India): Training data and test data\n",
    "\n",
    "Similarly, an equivalent division applies to the Google-sourced data:\n",
    "\n",
    "- Google (England): Training data and test data\n",
    "- Google (Australia): Training data and test data\n",
    "- Google (India): Training data and test data\n",
    "\n",
    "All datasets share the same attributes: `id`, a unique identifier for each entry, `text`, the content of the publication, and `sentiment_label`, the target variable for our analysis. The `sentiment_label` is binary, where `0` indicates a negative sentiment and `1` indicates a positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Initial Setup\n",
    "\n",
    "We begin by reading all 12 datasets. Since the distinction between training and test data is not relevant for our analysis, we first merge them, reducing the total to 6 datasets.\n",
    "\n",
    "To further facilitate analysis, we also create 3 additional datasets, grouping the data by country of origin. In this step, we combine Reddit and Google data while keeping separate datasets for England, Australia, and India Moreover, we also create 3 datasets, grouping the data by their source, this is Reddit or Google.\n",
    "\n",
    "In the end, we also created a global dataset, that is, with all the data we have available.\n",
    "\n",
    "In addition, we remove the `id` attribute at the beginning of our process. This decision was made to prevent inconsistencies, as some publications shared the same `id` across different datasets. Keeping this attribute could lead to ambiguity in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import spacy\n",
    "import os\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# nlp = spacy.load(\"en_core_web_sm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Reddit-sourced data \n",
    "reddit_uk_train = pd.read_json(\"data/reddit-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_train = pd.read_json(\"data/reddit-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_train = pd.read_json(\"data/reddit-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_uk_valid = pd.read_json(\"data/reddit-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_valid = pd.read_json(\"data/reddit-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_valid = pd.read_json(\"data/reddit-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# read Google-sourced data \n",
    "google_uk_train = pd.read_json(\"data/google-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_train = pd.read_json(\"data/google-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_train = pd.read_json(\"data/google-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_uk_valid = pd.read_json(\"data/google-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_valid = pd.read_json(\"data/google-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_valid = pd.read_json(\"data/google-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# merge Reddit-sourced data by country\n",
    "reddit_uk_union = pd.concat([reddit_uk_train, reddit_uk_valid], ignore_index=True)\n",
    "reddit_au_union = pd.concat([reddit_au_train, reddit_au_valid], ignore_index=True)\n",
    "reddit_in_union = pd.concat([reddit_in_train, reddit_in_valid], ignore_index=True)\n",
    "\n",
    "# merge Google-sourced data by country\n",
    "google_uk_union = pd.concat([google_uk_train, google_uk_valid], ignore_index=True)\n",
    "google_au_union = pd.concat([google_au_train, google_au_valid], ignore_index=True)\n",
    "google_in_union = pd.concat([google_in_train, google_in_valid], ignore_index=True)\n",
    "\n",
    "# merge data by country\n",
    "uk_union = pd.concat([reddit_uk_union, google_uk_union], ignore_index=True)\n",
    "au_union = pd.concat([reddit_au_union, google_au_union], ignore_index=True)\n",
    "in_union = pd.concat([reddit_in_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge data by source\n",
    "reddit_union = pd.concat([reddit_uk_union, reddit_au_union, reddit_in_union], ignore_index=True)\n",
    "google_union = pd.concat([google_uk_union, google_au_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge all data\n",
    "global_union = pd.concat([reddit_union, google_union])\n",
    "\n",
    "test_global = global_union.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Class Distribution by Source\n",
    "\n",
    "We begin by comparing the number of entries from Reddit and Google. Our analysis shows that both sources contain approximately the same number of entries.\n",
    "\n",
    "Next, we analyzed the distribution of the sentiment class in both sources. This analysis reveals that Reddit data is predominantly negative, while Google data is mostly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From Reddit there are \" + str(len(reddit_union)))\n",
    "print(\"From Google there are \" + str(len(google_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by source\n",
    "reddit_counts = reddit_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "google_counts = google_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "reddit_counts.columns = [\"sentiment_label\", \"Reddit\"]\n",
    "google_counts.columns = [\"sentiment_label\", \"Google\"]\n",
    "df = pd.merge(reddit_counts, google_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df.melt(id_vars=\"sentiment_label\", var_name=\"Source\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Source\", palette=[\"#1f77b4\", \"#ff7f0e\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of Sentiment Label Distribution: Reddit vs Google\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Source\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Class Distribution by Country\n",
    "\n",
    "Similarly to the source analysis, we also examined the distribution of the data across countries, as well as the balance of the target class distribution.\n",
    "\n",
    "As shown below, the datasets have roughly the same number of entries, and the target class is approximately evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From UK there are \" + str(len(uk_union)))\n",
    "print(\"From AU there are \" + str(len(au_union)))\n",
    "print(\"From IN there are \" + str(len(in_union)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by country\n",
    "uk_counts = uk_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "au_counts = au_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "in_counts = in_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "uk_counts.columns = [\"sentiment_label\", \"UK\"]\n",
    "au_counts.columns = [\"sentiment_label\", \"AU\"]\n",
    "in_counts.columns = [\"sentiment_label\", \"IN\"]\n",
    "df_counts = pd.merge(uk_counts, au_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_counts = pd.merge(df_counts, in_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df_counts.melt(id_vars=\"sentiment_label\", var_name=\"Country\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Country\", palette=[\"#1f77b4\", \"#ff7f0e\", \"#2f8c1f\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of Sentiment Label Distribution: UK vs AU vs IN\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Word Distribution by Source\n",
    "\n",
    "Next, we analyze the distribution of words by source, focusing on the top 10 words. As shown, despite the words appearing in different positions across the sources, there are 3 common words. This low number may be due to the topic of the posts, which can be very different between platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate and plot TF-IDF\n",
    "def plot_tfidf(dataset, title):\n",
    "    \n",
    "    # get text column\n",
    "    texts = dataset[\"text\"]  \n",
    "    \n",
    "    # start TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=20)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)   \n",
    "    terms = vectorizer.get_feature_names_out()   \n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)   \n",
    "    sum_tfidf = tfidf_df.sum(axis=0)   \n",
    "    sorted_tfidf = sum_tfidf.sort_values(ascending=False)\n",
    "    \n",
    "    # plot graph\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    sorted_tfidf.head(10).plot(kind='bar')\n",
    "    plt.title(title)\n",
    "    plt.ylabel('TF-IDF Score')\n",
    "    plt.xlabel('Words')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.show()\n",
    "    \n",
    "    return sorted_tfidf.head(10).index.tolist()\n",
    "\n",
    "def plot_wordcolud(dataset):\n",
    "    wordcloud = WordCloud().generate(\" \".join(dataset[\"text\"].dropna().astype(str)))\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_top = plot_tfidf(reddit_union, 'Top 10 TF-IDF Words for Reddit Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(reddit_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_top = plot_tfidf(google_union, 'Top 10 TF-IDF Words for Google Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(google_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_top = list(set(reddit_top) & set(google_top))\n",
    "\n",
    "print(\"There are \" + str(len(comumn_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Word Distribution by Country\n",
    "\n",
    "We also analyzed the distribution of words by country. As shown, the vocabulary does not seem to vary significantly across countries, as the number of common words between two datasets ranges from 7 to 8 out of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_top = plot_tfidf(uk_union, 'Top 10 TF-IDF Words for UK Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(uk_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_top = plot_tfidf(au_union, 'Top 10 TF-IDF Words for AU Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(au_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_top = plot_tfidf(in_union, 'Top 10 TF-IDF Words for IN Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(in_union)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_uk_au_top = list(set(uk_top) & set(au_top))\n",
    "comumn_au_in_top = list(set(au_top) & set(in_top))\n",
    "comumn_in_uk_top = list(set(in_top) & set(uk_top))\n",
    "\n",
    "print(\"UK and AU\")\n",
    "print(\"There are \" + str(len(comumn_uk_au_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_uk_au_top))\n",
    "\n",
    "print(\"\\nAU and IN\")\n",
    "print(\"There are \" + str(len(comumn_au_in_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_au_in_top))\n",
    "\n",
    "print(\"\\nIN and UK\")\n",
    "print(\"There are \" + str(len(comumn_in_uk_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_in_uk_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Word Distribuition by Class\n",
    "\n",
    "We also analyzed the word distribution by class, separating the publications based on their connotation, positive or negative.\n",
    "\n",
    "In positive publications, words like \"good,\" \"nice,\" and \"great\" stand out, which aligns with their typical use in expressing positive sentiments.\n",
    "\n",
    "Conversely, in negative publications, these words are noticeably less common. While \"good\" still appears frequently, it is not as prominent as in positive posts. Additionally, the word \"n't\" is highly recurrent, highlighting its role in negation and negative sentiment formation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(global_union[global_union[\"sentiment_label\"] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_wordcolud(global_union[global_union[\"sentiment_label\"] == 1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Pre-Processing\n",
    "In the data preprocessing step, we remove all characters that are not alphabetical or whitespace. Additionally, we convert the text to lowercase and eliminate consecutive spaces. After cleaning the text, we perform tokenization and stemming. Finally, we remove words from the stopwords list, except for those with negation, as they are crucial for our classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.word_tokenize\n",
    "stemmer = nltk.PorterStemmer()\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "stop_words.remove(\"no\")\n",
    "stop_words.remove(\"not\")\n",
    "stop_words.remove(\"nor\")\n",
    "stop_words.remove(\"t\")\n",
    "\n",
    "# remove words related with negation\n",
    "stop_words_remove = {\"no\", \"not\", \"nor\", \"t\"}\n",
    "stop_words.difference_update(stop_words_remove)\n",
    "\n",
    "def text_pre_processing(dataset):\n",
    "    dataset['text'] = dataset[\"text\"].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x).strip())\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(str.lower)\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "    dataset['text'] = dataset['text'].apply(tokenizer)\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [stemmer.stem(w) for w in x])\n",
    "    dataset['text'] = dataset['text'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    dataset['text'] = [\" \".join(text) for text in dataset[\"text\"]]\n",
    "    \n",
    "def metrics(y_test, y_pred):\n",
    "    [[tp, fp], [tn, fn]] = (confusion_matrix(y_test, y_pred))\n",
    "    print(\"         True  False\")\n",
    "    print(\"positive  \" + str(tp) + \"   \" + str(fp))\n",
    "    print(\"negative  \" + str(tn) + \"   \" + str(fn))\n",
    "    print(f\"\\nAccuracy {accuracy_score(y_test, y_pred):.2f} Precision {precision_score(y_test, y_pred):.2f} Recall {recall_score(y_test, y_pred):.2f} F1 Score {f1_score(y_test, y_pred):.2f}\")\n",
    "\n",
    "text_pre_processing(uk_union)\n",
    "text_pre_processing(au_union)\n",
    "text_pre_processing(in_union)\n",
    "text_pre_processing(global_union)\n",
    "\n",
    "print(uk_union['text'].iloc[0])\n",
    "print(len(uk_union['text'].iloc[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data_prepared\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "uk_union.to_csv(os.path.join(folder_path, \"uk_union.csv\"), index=False)\n",
    "au_union.to_csv(os.path.join(folder_path, \"au_union.csv\"), index=False)\n",
    "in_union.to_csv(os.path.join(folder_path, \"in_union.csv\"), index=False)\n",
    "\n",
    "global_union.to_csv(os.path.join(folder_path, \"global_union.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 - Features from Bag-of-Words\n",
    "\n",
    "The simplest way to accomplish this is by creating a Bag-of-Words model, which ignores the sequence of words. To do this, we convert a collection of text documents into a matrix of token counts. We then use this data to classify the test set using a Multinomial Naive Bayes classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "corpus = global_union[\"text\"].dropna().astype(str).tolist()\n",
    "\n",
    "# split data in training and testing\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = global_union['sentiment_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "\n",
    "# classify using Multinomial Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_nb = clf.predict(X_test)\n",
    "print(\"Multinomial Naive Bayes\\n\")\n",
    "metrics(y_test, y_pred_nb)\n",
    "\n",
    "# classify using Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_log = clf.predict(X_test)\n",
    "print(\"\\nLogistic Regression\\n\")\n",
    "metrics(y_test, y_pred_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=None)\n",
    "corpus = global_union[\"text\"].dropna().astype(str).tolist()\n",
    "\n",
    "# split data in training and testing\n",
    "X = vectorizer.fit_transform(corpus).toarray()\n",
    "y = global_union['sentiment_label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n",
    "\n",
    "# classify using Multinomial Naive Bayes\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_nb = clf.predict(X_test)\n",
    "print(\"Multinomial Naive Bayes\\n\")\n",
    "metrics(y_test, y_pred_nb)\n",
    "\n",
    "# classify using Logistic Regression\n",
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred_log = clf.predict(X_test)\n",
    "print(\"\\nLogistic Regression\\n\")\n",
    "metrics(y_test, y_pred_log)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - VADER\n",
    "\n",
    "Something about vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "y_pred = []\n",
    "indices = sorted(list(y_test.index))\n",
    "print(len(indices))\n",
    "\n",
    "for idx in indices:\n",
    "    comment = corpus[idx]\n",
    "    y_pred.append(1 if analyzer.polarity_scores(comment)['compound'] > 0 else 0)\n",
    "\n",
    "print(len(y_test), len(y_pred))\n",
    "\n",
    "[[tp, fp], [tn, fn]] = (confusion_matrix(y_test, y_pred))\n",
    "print(\"Confusion Matrix :\\n\")\n",
    "print(\"         True  False\")\n",
    "print(\"positive  \" + str(tp) + \"   \" + str(fp))\n",
    "print(\"negative  \" + str(tn) + \"   \" + str(fn))\n",
    "\n",
    "print(\"\\n\")\n",
    "print(f\"Accuracy   {accuracy_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Precision  {precision_score(y_test, y_pred):.2f}\")\n",
    "print(f\"Recall     {recall_score(y_test, y_pred):.2f}\")\n",
    "print(f\"F1 Score   {f1_score(y_test, y_pred):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import contractions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "x1_list, x2_list, x3_list, x4_list, x5_list, x6_list = [], [], [], [], [], []\n",
    "\n",
    "word_info = dict()\n",
    "\n",
    "pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\", \"you\", \"your\", \"yours\"}\n",
    "\n",
    "def text_contractions(dataset):\n",
    "    dataset['text'] = dataset['text'].apply(contractions.fix)\n",
    "    dataset['text'] = dataset[\"text\"].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x).strip())\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(str.lower)\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip()) \n",
    "\n",
    "text_contractions(test_global)\n",
    "\n",
    "for line in test_global[\"text\"]:\n",
    "    words = line.split()\n",
    "    x1 = 0\n",
    "    x2 = 0\n",
    "    for word in words:\n",
    "        if word in word_info:\n",
    "            if word_info[word] == 1:\n",
    "                x1 += 1\n",
    "            elif word_info[word] == -1:\n",
    "                x2 += 1\n",
    "        else:\n",
    "            neg_polarity = sia.polarity_scores(word)[\"neg\"]\n",
    "            pos_polarity = sia.polarity_scores(word)[\"pos\"]\n",
    "            cpd_polarity = sia.polarity_scores(word)[\"compound\"]\n",
    "\n",
    "            if cpd_polarity > 0.05 and pos_polarity > neg_polarity:\n",
    "                word_info[word] = 1\n",
    "                x1 += 1\n",
    "            elif cpd_polarity < -0.05 and pos_polarity < neg_polarity:\n",
    "                word_info[word] = -1\n",
    "                x2 += 1\n",
    "            else:\n",
    "                word_info[word] = 0\n",
    "\n",
    "    x3 = 1 if \"no\" in words else 0\n",
    "    x4 = sum(1 for word in words if word in pronouns)\n",
    "    x5 = 1 if \"!\" in words else 0\n",
    "    x6 = math.log(len(words) + 1)\n",
    "\n",
    "    \n",
    "    x1_list.append(x1)\n",
    "    x2_list.append(x2)\n",
    "    x3_list.append(x3)\n",
    "    x4_list.append(x4)\n",
    "    x5_list.append(x5)\n",
    "    x6_list.append(x6)\n",
    "\n",
    "test_global[\"x1\"] = x1_list\n",
    "test_global[\"x2\"] = x2_list\n",
    "test_global[\"x3\"] = x3_list\n",
    "test_global[\"x4\"] = x4_list\n",
    "test_global[\"x5\"] = x5_list\n",
    "test_global[\"x6\"] = x6_list\n",
    "    \n",
    "\n",
    "test_global.to_csv(os.path.join(folder_path, \"test_global.csv\"), index=False)\n",
    "\n",
    "\n",
    "X = test_global[[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\"]]\n",
    "y = test_global[\"sentiment_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Learned coefficients:\", model.coef_)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "[[tp, fp], [tn, fn]] = (confusion_matrix(y_test, y_pred))\n",
    "print(\"Confusion Matrix :\\n\")\n",
    "print(\"         True  False\")\n",
    "print(\"positive  \" + str(tp) + \"   \" + str(fp))\n",
    "print(\"negative  \" + str(tn) + \"   \" + str(fn))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
