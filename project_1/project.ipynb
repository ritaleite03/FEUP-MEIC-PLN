{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 -  Text Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Provenance and Characteristics\n",
    "\n",
    "The dataset we will be working with consists of publications sourced from Reddit and Google, authored by individuals from England, Australia, and India. \n",
    "\n",
    "The Reddit-sourced data is divided as follows:\n",
    "\n",
    "- Reddit (England): Training data and test data\n",
    "- Reddit (Australia): Training data and test data\n",
    "- Reddit (India): Training data and test data\n",
    "\n",
    "Similarly, an equivalent division applies to the Google-sourced data:\n",
    "\n",
    "- Google (England): Training data and test data\n",
    "- Google (Australia): Training data and test data\n",
    "- Google (India): Training data and test data\n",
    "\n",
    "All datasets share the same attributes: `id`, a unique identifier for each entry, `text`, the content of the publication, and `sentiment_label`, the target variable for our analysis. The `sentiment_label` is binary, where `0` indicates a negative sentiment and `1` indicates a positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Initial Setup\n",
    "\n",
    "We begin by reading all 12 datasets. Since the distinction between training and test data is not relevant for our analysis, we first merge them, reducing the total to 6 datasets.\n",
    "\n",
    "To further facilitate analysis, we also create 3 additional datasets, grouping the data by country of origin. In this step, we combine Reddit and Google data while keeping separate datasets for England, Australia, and India Moreover, we also create 3 datasets, grouping the data by their source, this is Reddit or Google.\n",
    "\n",
    "In the end, we also created a global dataset, that is, with all the data we have available.\n",
    "\n",
    "In addition, we remove the `id` attribute at the beginning of our process. This decision was made to prevent inconsistencies, as some publications shared the same `id` across different datasets. Keeping this attribute could lead to ambiguity in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import gensim\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.calibration import cross_val_predict\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from wordcloud import WordCloud\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# import spacy\n",
    "# nlp = spacy.load(\"en_core_web_sm\")\n",
    "from scipy import stats\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Reddit-sourced data \n",
    "reddit_uk_train = pd.read_json(\"data/reddit-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_train = pd.read_json(\"data/reddit-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_train = pd.read_json(\"data/reddit-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_uk_valid = pd.read_json(\"data/reddit-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_valid = pd.read_json(\"data/reddit-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_valid = pd.read_json(\"data/reddit-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# read Google-sourced data \n",
    "google_uk_train = pd.read_json(\"data/google-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_train = pd.read_json(\"data/google-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_train = pd.read_json(\"data/google-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_uk_valid = pd.read_json(\"data/google-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_valid = pd.read_json(\"data/google-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_valid = pd.read_json(\"data/google-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# merge Reddit-sourced data by country\n",
    "reddit_uk_union = pd.concat([reddit_uk_train, reddit_uk_valid], ignore_index=True)\n",
    "reddit_au_union = pd.concat([reddit_au_train, reddit_au_valid], ignore_index=True)\n",
    "reddit_in_union = pd.concat([reddit_in_train, reddit_in_valid], ignore_index=True)\n",
    "\n",
    "# merge Google-sourced data by country\n",
    "google_uk_union = pd.concat([google_uk_train, google_uk_valid], ignore_index=True)\n",
    "google_au_union = pd.concat([google_au_train, google_au_valid], ignore_index=True)\n",
    "google_in_union = pd.concat([google_in_train, google_in_valid], ignore_index=True)\n",
    "\n",
    "# merge data by country\n",
    "uk_union = pd.concat([reddit_uk_union, google_uk_union], ignore_index=True)\n",
    "au_union = pd.concat([reddit_au_union, google_au_union], ignore_index=True)\n",
    "in_union = pd.concat([reddit_in_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge data by source\n",
    "reddit_union = pd.concat([reddit_uk_union, reddit_au_union, reddit_in_union], ignore_index=True)\n",
    "google_union = pd.concat([google_uk_union, google_au_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge all data\n",
    "global_union = pd.concat([reddit_union, google_union])\n",
    "test_global = global_union.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(values, labels, title=\"Pie Chart\"):\n",
    "    plt.figure(figsize=(2 * len(values), len(values)))\n",
    "    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)   \n",
    "    plt.title(title)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie_chart([len(global_union[global_union[\"sentiment_label\"] == 0]), len(global_union[global_union[\"sentiment_label\"] == 1])], [\"Negative\", \"Positive\"], title=\"Entries by Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Class Distribution by Source\n",
    "\n",
    "We begin by comparing the number of entries from Reddit and Google. Our analysis shows that both sources contain approximately the same number of entries.\n",
    "\n",
    "Next, we analyzed the distribution of the sentiment class in both sources. This analysis reveals that Reddit data is predominantly negative, while Google data is mostly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From Reddit there are \" + str(len(reddit_union)))\n",
    "print(\"From Google there are \" + str(len(google_union)))\n",
    "plot_pie_chart([len(reddit_union), len(google_union)], [\"Reddit\", \"Google\"], title=\"Entries by Source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by source\n",
    "reddit_counts = reddit_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "google_counts = google_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "reddit_counts.columns = [\"sentiment_label\", \"Reddit\"]\n",
    "google_counts.columns = [\"sentiment_label\", \"Google\"]\n",
    "df = pd.merge(reddit_counts, google_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df.melt(id_vars=\"sentiment_label\", var_name=\"Source\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Source\", palette=[\"#1f77b4\", \"#ff7f0e\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of target label by source: Reddit vs Google\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Source\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Class Distribution by Country\n",
    "\n",
    "Similarly to the source analysis, we also examined the distribution of the data across countries, as well as the balance of the target class distribution.\n",
    "\n",
    "As shown below, the datasets have roughly the same number of entries, and the target class is approximately evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From UK there are \" + str(len(uk_union)))\n",
    "print(\"From AU there are \" + str(len(au_union)))\n",
    "print(\"From IN there are \" + str(len(in_union)))\n",
    "plot_pie_chart([len(uk_union), len(au_union), len(in_union)], [\"UK\", \"AU\", \"IN\"], title=\"Entries by Country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by country\n",
    "uk_counts = uk_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "au_counts = au_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "in_counts = in_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "uk_counts.columns = [\"sentiment_label\", \"UK\"]\n",
    "au_counts.columns = [\"sentiment_label\", \"AU\"]\n",
    "in_counts.columns = [\"sentiment_label\", \"IN\"]\n",
    "df_counts = pd.merge(uk_counts, au_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_counts = pd.merge(df_counts, in_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df_counts.melt(id_vars=\"sentiment_label\", var_name=\"Country\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Country\", palette=[\"#1f77b4\", \"#ff7f0e\", \"#2f8c1f\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of target label by country: UK vs AU vs IN\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Word Distribution by Source\n",
    "\n",
    "Next, we analyze the distribution of words by source, focusing on the top 10 words. As shown, despite the words appearing in different positions across the sources, there are 3 common words. This low number may be due to the topic of the posts, which can be very different between platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate and plot TF-IDF\n",
    "def plot_tfidf(dataset, title):\n",
    "    \n",
    "    # get text column\n",
    "    texts = dataset[\"text\"]  \n",
    "    \n",
    "    # start TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)   \n",
    "    terms = vectorizer.get_feature_names_out()   \n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)   \n",
    "    sum_tfidf = tfidf_df.sum(axis=0)   \n",
    "    sorted_tfidf = sum_tfidf.sort_values(ascending=False)\n",
    "    \n",
    "    # plot graph\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(sorted_tfidf)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return sorted_tfidf.head(10).index.tolist()\n",
    "\n",
    "def plot_wordcolud(dataset):\n",
    "    wordcloud = WordCloud().generate(\" \".join(dataset[\"text\"].dropna().astype(str)))\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_top = plot_tfidf(reddit_union, 'Top 100 TF-IDF Words for Reddit Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_top = plot_tfidf(google_union, 'Top 100 TF-IDF Words for Google Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_top = list(set(reddit_top) & set(google_top))\n",
    "\n",
    "print(\"There are \" + str(len(comumn_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Word Distribution by Country\n",
    "\n",
    "We also analyzed the distribution of words by country. As shown, the vocabulary does not seem to vary significantly across countries, as the number of common words between two datasets ranges from 7 to 8 out of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_top = plot_tfidf(uk_union, 'Top 100 TF-IDF Words for UK Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_top = plot_tfidf(au_union, 'Top 100 TF-IDF Words for AU Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_top = plot_tfidf(in_union, 'Top 100 TF-IDF Words for IN Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_uk_au_top = list(set(uk_top) & set(au_top))\n",
    "comumn_au_in_top = list(set(au_top) & set(in_top))\n",
    "comumn_in_uk_top = list(set(in_top) & set(uk_top))\n",
    "\n",
    "print(\"UK and AU\")\n",
    "print(\"There are \" + str(len(comumn_uk_au_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_uk_au_top))\n",
    "\n",
    "print(\"\\nAU and IN\")\n",
    "print(\"There are \" + str(len(comumn_au_in_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_au_in_top))\n",
    "\n",
    "print(\"\\nIN and UK\")\n",
    "print(\"There are \" + str(len(comumn_in_uk_top)) + \" comumn words.\")\n",
    "print(\"They are : \" + str(comumn_in_uk_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Pre-Processing\n",
    "In the data preprocessing step, we remove all characters that are not alphabetical or whitespace. Additionally, we convert the text to lowercase and eliminate consecutive spaces. After cleaning the text, we perform tokenization and stemming. Finally, we remove words from the stopwords list, except for those with negation, as they are crucial for our classification task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.word_tokenize\n",
    "stemmer = nltk.PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "stop_words.remove(\"no\")\n",
    "stop_words.remove(\"not\")\n",
    "stop_words.remove(\"nor\")\n",
    "stop_words.remove(\"t\")\n",
    "\n",
    "# remove words related with negation\n",
    "stop_words_remove = {\"no\", \"not\", \"nor\", \"t\"}\n",
    "stop_words.difference_update(stop_words_remove)\n",
    "\n",
    "def text_pre_processing(dataset):\n",
    "    # remove all the caracteres that do not belong to the alphabet and are not a whitespace\n",
    "    dataset['text'] = dataset[\"text\"].apply(lambda x: re.sub(r'[^\\x00-\\x7F]|[^a-zA-Z ]', ' ', x).strip())\n",
    "    # converte all caracteres to lowercase\n",
    "    dataset[\"text_processed\"] = dataset[\"text\"].apply(str.lower)\n",
    "    # remove multiple whitespaces\n",
    "    dataset[\"text_processed\"] = dataset[\"text_processed\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "    # apply tokenization\n",
    "    dataset['text_processed'] = dataset['text_processed'].apply(tokenizer)   \n",
    "    # remove stopwords, mantaining words like 'no', 'not', 'nor', 't'\n",
    "    dataset['text_processed'] = dataset['text_processed'].apply(lambda x: [word for word in x if word not in stop_words])\n",
    "    # # apply stemization\n",
    "    # # dataset['text_processed'] = dataset['text_processed'].apply(lambda x: [stemmer.stem(w) for w in x])\n",
    "    # apply lemmazation\n",
    "    dataset['text_processed'] = dataset['text_processed'].apply(lambda x: [lemmatizer.lemmatize(w) for w in x])    \n",
    "    # # convert \"t\" to \"not\"\n",
    "    # # dataset['text_processed'] = dataset['text_processed'].apply(lambda x: ['not' if word == 't' else word for word in x])\n",
    "    # join words from the list in a sentence\n",
    "    dataset['text_processed'] = [\" \".join(text) for text in dataset[\"text_processed\"]]\n",
    "    \n",
    "def metrics(y_test, y_pred, time):\n",
    "    [[tp, fp], [tn, fn]] = (confusion_matrix(y_test, y_pred))\n",
    "    print(\"                       positive   negative\")\n",
    "    print(\"predicted positive     \" + str(tp) + (\" \" * (len(str(tp)) - 1)) + \"      \" + str(fp))\n",
    "    print(\"predicted negative     \" + str(tn) + (\" \" * (len(str(tn)) - 1)) + \"      \" + str(fn))\n",
    "    print(f\"\\nAccuracy {accuracy_score(y_test, y_pred):.2f} Precision {precision_score(y_test, y_pred):.2f} Recall {recall_score(y_test, y_pred):.2f} F1 Score {f1_score(y_test, y_pred):.2f} Time {time:.2f}\") \n",
    "\n",
    "text_pre_processing(uk_union)\n",
    "text_pre_processing(au_union)\n",
    "text_pre_processing(in_union)\n",
    "text_pre_processing(global_union)\n",
    "\n",
    "print(uk_union['text_processed'].iloc[0])\n",
    "print(len(uk_union['text_processed'].iloc[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we have made these modifications to the datasets, we save them to our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = \"data_prepared\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "\n",
    "uk_union.to_csv(os.path.join(folder_path, \"uk_union.csv\"), index=False)\n",
    "au_union.to_csv(os.path.join(folder_path, \"au_union.csv\"), index=False)\n",
    "in_union.to_csv(os.path.join(folder_path, \"in_union.csv\"), index=False)\n",
    "\n",
    "global_union.to_csv(os.path.join(folder_path, \"global_union.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Representation Technique\n",
    "Subsequently, we applied a range of feature representation techniques, exploring both sparse and dense vector approaches.\n",
    "\n",
    "In relation to sparse vectors:\n",
    "\n",
    "- **CountVectorizer**: the text is converted into a \"bag of words\" (BoW), where the order of the words is not considered. The focus is solely on how many times each word appears in the document, without taking into account the sequence or context in which it occurs.\n",
    "\n",
    "- **TfidfVectorizer**: is similar to CountVectorizer, but with an important difference: it adjusts the word counts by considering not only the frequency of a word in a specific text but also its relative importance in the larger set of documents.\n",
    "\n",
    "- **ngram_range in TfidfVectorizer**: the `ngram_range` parameter in TfidfVectorizer allows the model to capture n-grams. By setting `ngram_range` to `(1,2)`, the model will consider both unigrams (individual words) and bigrams (pairs of consecutive words), allowing it to capture more contextual information between nearby words.\n",
    "\n",
    "In relation to dense vectors:\n",
    "\n",
    "- **Word2Vec**: used to generate dense vector representations of words in a continuous vector space, where words with similar meanings are close together in the vector space.\n",
    "\n",
    "- **GloVe**: is also a word embedding technique, but unlike Word2Vec, which is based on a predictive model, GloVe is based on word co-occurrence matrices in a corpus. It tries to capture the global relationships between words, making it effective for representing word meanings in broader contexts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = global_union[\"text_processed\"].dropna().astype(str).tolist()\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "print(\"Text's characteristic:\")\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))\n",
    "\n",
    "print(\"\\nRepresentation with sparse vectors:\\n\")\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "x_CountVectorizer = vectorizer.fit_transform(corpus)\n",
    "print(\"  - CountVectorizer's shape is \" + str(x_CountVectorizer.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None)\n",
    "x_TfidfVectorizer = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizer's shape is \" + str(x_TfidfVectorizer.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(1,2)) # unigram and bigram\n",
    "x_TfidfVectorizerUB = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizerUB's shape is\" + str(x_TfidfVectorizerUB.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(2,2)) # bigram\n",
    "x_TfidfVectorizerB = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizerB's shape is \" + str(x_TfidfVectorizerB.shape))\n",
    "\n",
    "# word embedings\n",
    "print(\"\\nRepresentation with dense vectors:\\n\")\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len):\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:\n",
    "        try:\n",
    "            vec.extend(embeddings.get_vector(tokens[i]))\n",
    "            n += 1\n",
    "        except KeyError: True\n",
    "        finally: i += 1\n",
    "    \n",
    "    for j in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embeddings.vector_size,))\n",
    "        \n",
    "    return vec\n",
    "\n",
    "def text_to_vector_glove(embeddings, text, sequence_len, embedding_dim=100):\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    while i < len(tokens) and n < sequence_len:\n",
    "        if tokens[i] in embeddings:\n",
    "            vec.extend(embeddings[tokens[i]])\n",
    "            n += 1\n",
    "        i += 1\n",
    "   \n",
    "    for _ in range(sequence_len - n):\n",
    "        vec.extend(np.zeros(embedding_dim,))\n",
    "        \n",
    "    return vec\n",
    "\n",
    "# text_og = global_union[\"text\"].copy()\n",
    "# text_og = text_og.apply(lambda x: re.sub(r'[^\\x00-\\x7F]|[^a-zA-Z ]', ' ', x).strip())\n",
    "# text_og = text_og.apply(str.lower)\n",
    "# text_og = text_og.apply(lambda x: re.sub(r'\\s+', ' ', x).strip())\n",
    "# corpus_og = text_og.dropna().astype(str).tolist()\n",
    "corpus_og = corpus\n",
    "\n",
    "model = gensim.models.Word2Vec(corpus_og, vector_size=100, window=3, min_count=2, workers=10, sg=1)\n",
    "embeddings_corpus = []\n",
    "for c in corpus:\n",
    "    embeddings_corpus.append(text_to_vector(model.wv, c, 10))\n",
    "\n",
    "x_Word2Vec = np.array(embeddings_corpus)\n",
    "print(\"  - Word2Vec's shape is \" + str(x_Word2Vec.shape))\n",
    "\n",
    "word_vectors = {}\n",
    "with open(\"data_prepared/glove.6B.100d.txt\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], dtype=\"float32\")\n",
    "        word_vectors[word] = vector\n",
    "\n",
    "embeddings_corpus_glove = [text_to_vector_glove(word_vectors, c, 10) for c in corpus]\n",
    "\n",
    "x_GloVe = np.array(embeddings_corpus_glove)\n",
    "print(\"  - GloVe's shape is \" + str(x_GloVe.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'LogisticRegression': {\n",
    "        'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n",
    "        'solver': ['liblinear', 'saga', 'lbfgs'],\n",
    "        'max_iter': [100, 200, 500]\n",
    "    },\n",
    "    'SGDClassifier': {\n",
    "        'loss': ['hinge', 'log'],\n",
    "        'penalty': ['l1', 'l2', 'elasticnet'],\n",
    "        'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "    }\n",
    "}\n",
    "\n",
    "def parameter_tuning(clf, x_predict, y_predict):\n",
    "    grid_search = GridSearchCV(clf, parameters[str(clf.__class__.__name__)], cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(x_predict, y_predict)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "    \n",
    "\n",
    "def predict_nb(x, y):\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    y_pred = clf.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "\n",
    "def predict_log(x, y):\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state=42)\n",
    "    clf = LogisticRegression(max_iter=1000)\n",
    "    # parameter tunning and training \n",
    "    model, param = parameter_tuning(clf, X_train, y_train)\n",
    "    start_time = time.time()   \n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    print(f\"\\nBets parameters: {param}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "    \n",
    "def predict_sgd(x, y):\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.30, random_state=42)\n",
    "    clf = SGDClassifier(max_iter=1000, tol=1e-3)\n",
    "    # parameter tunning and training \n",
    "    model, param = parameter_tuning(clf, X_train, y_train)\n",
    "    start_time = time.time()   \n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    print(f\"\\nBets parameters: {param}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline\n",
    "\n",
    "Initially, we developed a baseline model where the text was not subjected to any preprocessing, and the feature representation was based on a Bag of Words approach. For classification, we utilized the Multinomial Naive Bayes (MultinomialNB) algorithm.\n",
    "\n",
    "Comparing the size of the feature space in our baseline model, which utilizes raw text without any preprocessing, we observe that it is significantly larger than the feature space of the preprocessed text. Specifically, the baseline model results in **28,874 features**, whereas the preprocessed text yields **16,551 features**, making the feature space approximately **1.7 times larger** in the absence of preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None, lowercase=False)\n",
    "x_baseline = vectorizer.fit_transform(global_union[\"text\"].dropna().astype(str).tolist())\n",
    "print(\"Baseline - \" + str(x_baseline.shape))\n",
    "predict_nb(x_baseline, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Multinomial Naive Bayes\n",
    "\n",
    "Comparing the results obtained with the preprocessed text to those of the baseline model, we observe that the differences in error rates are minimal. However, the models using preprocessed text demonstrate **substantially faster training times**.\n",
    "\n",
    "We can also observe that, regarding the use of n-grams, the model utilizing only bigrams yields the poorest results. In contrast, the model that combines unigrams and bigrams achieves better performance. This approach strikes a good balance, potentially improving the model's ability to handle cases involving negation more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_nb(x_CountVectorizer, global_union['sentiment_label'])\n",
    "predict_nb(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "predict_nb(x_TfidfVectorizerUB, global_union['sentiment_label'])\n",
    "predict_nb(x_TfidfVectorizerB, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Logistic Regression\n",
    "\n",
    "Based on the results obtained previously, we decided to discontinue testing models with features extracted using TfidfVectorizer and bigrams. In addition, we explored feature representations using dense vectors, generated through two different methods: **Word2Vec** and **GloVe**.  \n",
    "\n",
    "When comparing Logistic Regression with Multinomial Naive Bayes, we observed that Logistic Regression consistently outperforms Naive Bayes in terms of accuracy. However, it requires a longer training time due to its iterative optimization process. In contrast, Naive Bayes is significantly faster, but its performance is generally inferior.  \n",
    "\n",
    "Additionally, we found that models trained with sparse vector representations tend to yield better results than those using dense vector representations. Notably, Word2Vec-based features performed significantly worse, which may be attributed to the limited size of our dataset and vocabulary. This likely prevented the Word2Vec model from effectively capturing the semantic relationships between words, resulting in poorer feature representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_log(x_CountVectorizer, global_union['sentiment_label'])\n",
    "predict_log(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "predict_log(x_TfidfVectorizerUB, global_union['sentiment_label'])\n",
    "predict_log(x_Word2Vec, global_union['sentiment_label'])\n",
    "predict_log(x_GloVe, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Stochastic Gradient Descent (SGD) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_sgd(x_CountVectorizer, global_union['sentiment_label'])\n",
    "predict_sgd(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "predict_sgd(x_TfidfVectorizerUB, global_union['sentiment_label'])\n",
    "predict_sgd(x_Word2Vec, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 - VADER\n",
    "\n",
    "Something about vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "y_pred = []\n",
    "\n",
    "start_time = time.time()\n",
    "for comment in global_union['text']:\n",
    "    y_pred.append(1 if analyzer.polarity_scores(comment)['compound'] > 0 else 0)\n",
    "end_time = time.time()\n",
    "\n",
    "metrics(global_union['sentiment_label'], y_pred, end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import contractions\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "x1_list, x2_list, x3_list, x4_list, x5_list, x6_list = [], [], [], [], [], []\n",
    "\n",
    "word_info = dict()\n",
    "\n",
    "pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\", \"you\", \"your\", \"yours\"}\n",
    "\n",
    "def text_contractions(dataset):\n",
    "    dataset['text'] = dataset['text'].apply(contractions.fix)\n",
    "    dataset['text'] = dataset[\"text\"].apply(lambda x: re.sub(r'[^a-zA-Z]', ' ', x).strip())\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(str.lower)\n",
    "    dataset[\"text\"] = dataset[\"text\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip()) \n",
    "\n",
    "text_contractions(test_global)\n",
    "\n",
    "for line in test_global[\"text\"]:\n",
    "    words = line.split()\n",
    "    x1 = 0\n",
    "    x2 = 0\n",
    "    for word in words:\n",
    "        if word in word_info:\n",
    "            if word_info[word] == 1:\n",
    "                x1 += 1\n",
    "            elif word_info[word] == -1:\n",
    "                x2 += 1\n",
    "        else:\n",
    "            neg_polarity = sia.polarity_scores(word)[\"neg\"]\n",
    "            pos_polarity = sia.polarity_scores(word)[\"pos\"]\n",
    "            cpd_polarity = sia.polarity_scores(word)[\"compound\"]\n",
    "\n",
    "            if cpd_polarity > 0.05 and pos_polarity > neg_polarity:\n",
    "                word_info[word] = 1\n",
    "                x1 += 1\n",
    "            elif cpd_polarity < -0.05 and pos_polarity < neg_polarity:\n",
    "                word_info[word] = -1\n",
    "                x2 += 1\n",
    "            else:\n",
    "                word_info[word] = 0\n",
    "\n",
    "    x3 = 1 if \"no\" in words else 0\n",
    "    x4 = sum(1 for word in words if word in pronouns)\n",
    "    x5 = 1 if \"!\" in words else 0\n",
    "    x6 = math.log(len(words) + 1)\n",
    "\n",
    "    \n",
    "    x1_list.append(x1)\n",
    "    x2_list.append(x2)\n",
    "    x3_list.append(x3)\n",
    "    x4_list.append(x4)\n",
    "    x5_list.append(x5)\n",
    "    x6_list.append(x6)\n",
    "\n",
    "test_global[\"x1\"] = x1_list\n",
    "test_global[\"x2\"] = x2_list\n",
    "test_global[\"x3\"] = x3_list\n",
    "test_global[\"x4\"] = x4_list\n",
    "test_global[\"x5\"] = x5_list\n",
    "\n",
    "test_global[\"x6\"] = x6_list\n",
    "    \n",
    "\n",
    "test_global.to_csv(os.path.join(folder_path, \"test_global.csv\"), index=False)\n",
    "\n",
    "\n",
    "X = test_global[[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\"]]\n",
    "y = test_global[\"sentiment_label\"]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Learned coefficients:\", model.coef_)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n",
    "\n",
    "[[tp, fp], [tn, fn]] = (confusion_matrix(y_test, y_pred))\n",
    "print(\"Confusion Matrix :\\n\")\n",
    "print(\"         True  False\")\n",
    "print(\"positive  \" + str(tp) + \"   \" + str(fp))\n",
    "print(\"negative  \" + str(tn) + \"   \" + str(fn))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
