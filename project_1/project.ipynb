{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project 1 -  Text Classification Task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 - Data Provenance and Characteristics\n",
    "\n",
    "The dataset we will be working with consists of publications sourced from Reddit and Google, authored by individuals from England, Australia, and India. \n",
    "\n",
    "The Reddit-sourced data is divided as follows:\n",
    "\n",
    "- Reddit (England): Training data and test data\n",
    "- Reddit (Australia): Training data and test data\n",
    "- Reddit (India): Training data and test data\n",
    "\n",
    "Similarly, an equivalent division applies to the Google-sourced data:\n",
    "\n",
    "- Google (England): Training data and test data\n",
    "- Google (Australia): Training data and test data\n",
    "- Google (India): Training data and test data\n",
    "\n",
    "All datasets share the same attributes: `id`, a unique identifier for each entry, `text`, the content of the publication, and `sentiment_label`, the target variable for our analysis. The `sentiment_label` is binary, where `0` indicates a negative sentiment and `1` indicates a positive sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 - Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 - Initial Setup\n",
    "\n",
    "We begin by reading all 12 datasets. Since the distinction between training and test data is not relevant for our analysis, we first merge them, reducing the total to 6 datasets.\n",
    "\n",
    "To further facilitate analysis, we also create 3 additional datasets, grouping the data by country of origin. In this step, we combine Reddit and Google data while keeping separate datasets for England, Australia, and India. Moreover, we also create 3 datasets, grouping the data by their source, whcih is either Reddit or Google.\n",
    "\n",
    "In the end, we also created a global dataset, that is, with all the data we have available.\n",
    "\n",
    "In addition, we removed the `id` attribute at the beginning of our process. This decision was made to prevent inconsistencies, as some publications shared the same `id` across different datasets. Keeping this attribute could lead to ambiguity in the data.\n",
    "\n",
    "It is important to note that for this project, you need to download a file containing pre-trained embeddings. This file was not added directly to the repository due to its size. To download it, click on this [link](https://www.kaggle.com/datasets/danielwillgeorge/glove6b100dtxt), which will redirect you to a dataset available on Kaggle. Please put the file inside the directory `data_prepared`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import math\n",
    "import contractions\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, ParameterGrid\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression, SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read Reddit-sourced data \n",
    "reddit_uk_train = pd.read_json(\"data/reddit-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_train = pd.read_json(\"data/reddit-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_train = pd.read_json(\"data/reddit-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_uk_valid = pd.read_json(\"data/reddit-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_in_valid = pd.read_json(\"data/reddit-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "reddit_au_valid = pd.read_json(\"data/reddit-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# read Google-sourced data \n",
    "google_uk_train = pd.read_json(\"data/google-uk-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_train = pd.read_json(\"data/google-in-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_train = pd.read_json(\"data/google-au-train.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_uk_valid = pd.read_json(\"data/google-uk-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_in_valid = pd.read_json(\"data/google-in-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "google_au_valid = pd.read_json(\"data/google-au-valid.jsonl\", lines=True).drop(\"id\", axis=1)\n",
    "\n",
    "# merge Reddit-sourced data by country\n",
    "reddit_uk_union = pd.concat([reddit_uk_train, reddit_uk_valid], ignore_index=True)\n",
    "reddit_au_union = pd.concat([reddit_au_train, reddit_au_valid], ignore_index=True)\n",
    "reddit_in_union = pd.concat([reddit_in_train, reddit_in_valid], ignore_index=True)\n",
    "\n",
    "# merge Google-sourced data by country\n",
    "google_uk_union = pd.concat([google_uk_train, google_uk_valid], ignore_index=True)\n",
    "google_au_union = pd.concat([google_au_train, google_au_valid], ignore_index=True)\n",
    "google_in_union = pd.concat([google_in_train, google_in_valid], ignore_index=True)\n",
    "\n",
    "# merge data by country\n",
    "uk_union = pd.concat([reddit_uk_union, google_uk_union], ignore_index=True)\n",
    "au_union = pd.concat([reddit_au_union, google_au_union], ignore_index=True)\n",
    "in_union = pd.concat([reddit_in_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge data by source\n",
    "reddit_union = pd.concat([reddit_uk_union, reddit_au_union, reddit_in_union], ignore_index=True)\n",
    "google_union = pd.concat([google_uk_union, google_au_union, google_in_union], ignore_index=True)\n",
    "\n",
    "# merge all data\n",
    "global_union = pd.concat([reddit_union, google_union]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pie_chart(values, labels, title=\"Pie Chart\"):\n",
    "    plt.figure(figsize=(2 * len(values), len(values)))\n",
    "    plt.pie(values, labels=labels, autopct='%1.1f%%', startangle=90)   \n",
    "    plt.title(title)    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_pie_chart([len(global_union[global_union[\"sentiment_label\"] == 0]), len(global_union[global_union[\"sentiment_label\"] == 1])], [\"Negative\", \"Positive\"], title=\"Entries by Target\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 - Class Distribution by Source\n",
    "\n",
    "We began by comparing the number of entries from Reddit and Google. Our analysis shows that both sources contain approximately the same number of entries.\n",
    "\n",
    "Next, we analyzed the distribution of the sentiment class in both sources. This analysis revealed that Reddit data is predominantly negative, while Google data is mostly positive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From Reddit there are \" + str(len(reddit_union)) + \" entries\")\n",
    "print(\"From Google there are \" + str(len(google_union)) + \" entries\")\n",
    "plot_pie_chart([len(reddit_union), len(google_union)], [\"Reddit\", \"Google\"], title=\"Entries by Source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by source\n",
    "reddit_counts = reddit_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "google_counts = google_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "reddit_counts.columns = [\"sentiment_label\", \"Reddit\"]\n",
    "google_counts.columns = [\"sentiment_label\", \"Google\"]\n",
    "df = pd.merge(reddit_counts, google_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df.melt(id_vars=\"sentiment_label\", var_name=\"Source\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(4,3))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Source\", palette=[\"#1f77b4\", \"#ff7f0e\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of target label by source: Reddit vs Google\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Source\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 - Class Distribution by Country\n",
    "\n",
    "Similarly to the source analysis, we also examined the distribution of the data across countries, as well as the balance of the target class distribution.\n",
    "\n",
    "As shown below, the datasets have roughly the same number of entries, and the target class is approximately evenly distributed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"From UK there are \" + str(len(uk_union)) + \" entries\")\n",
    "print(\"From AU there are \" + str(len(au_union)) + \" entries\")\n",
    "print(\"From IN there are \" + str(len(in_union)) + \" entries\")\n",
    "plot_pie_chart([len(uk_union), len(au_union), len(in_union)], [\"UK\", \"AU\", \"IN\"], title=\"Entries by Country\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count distribuition of the target class by country\n",
    "uk_counts = uk_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "au_counts = au_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "in_counts = in_union[\"sentiment_label\"].value_counts().reset_index()\n",
    "\n",
    "# create dataframe\n",
    "uk_counts.columns = [\"sentiment_label\", \"UK\"]\n",
    "au_counts.columns = [\"sentiment_label\", \"AU\"]\n",
    "in_counts.columns = [\"sentiment_label\", \"IN\"]\n",
    "df_counts = pd.merge(uk_counts, au_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_counts = pd.merge(df_counts, in_counts, on=\"sentiment_label\", how=\"outer\")\n",
    "df_melted = df_counts.melt(id_vars=\"sentiment_label\", var_name=\"Country\", value_name=\"Count\")\n",
    "\n",
    "# create graph\n",
    "plt.figure(figsize=(6,4))\n",
    "sns.barplot(data=df_melted, x=\"sentiment_label\", y=\"Count\", hue=\"Country\", palette=[\"#1f77b4\", \"#ff7f0e\", \"#2f8c1f\"])\n",
    "plt.xlabel(\"Sentiment Label\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.title(\"Comparison of target label by country: UK vs AU vs IN\")\n",
    "plt.xticks(ticks=[0, 1], labels=[\"Negative (0)\", \"Positive (1)\"])\n",
    "plt.legend(title=\"Country\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 - Word Distribution by Source\n",
    "\n",
    "Next, we analyzed the distribution of words by source, focusing on the top 10 words. As shown, despite the words appearing in different positions across the sources, there are 3 common words. This low number may be due to the topic of the posts, which can be very different between platforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate and plot TF-IDF\n",
    "def plot_tfidf(dataset, title):\n",
    "    \n",
    "    # get text column\n",
    "    texts = dataset[\"text\"]  \n",
    "    \n",
    "    # start TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer(stop_words='english', max_features=100)\n",
    "    tfidf_matrix = vectorizer.fit_transform(texts)   \n",
    "    terms = vectorizer.get_feature_names_out()   \n",
    "    tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=terms)   \n",
    "    sum_tfidf = tfidf_df.sum(axis=0)   \n",
    "    sorted_tfidf = sum_tfidf.sort_values(ascending=False)\n",
    "    \n",
    "    # plot graph\n",
    "    wordcloud = WordCloud(width=800, height=400, background_color='white').generate_from_frequencies(sorted_tfidf)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation='bilinear')\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    \n",
    "    return sorted_tfidf.head(10).index.tolist()\n",
    "\n",
    "def plot_wordcolud(dataset):\n",
    "    wordcloud = WordCloud().generate(\" \".join(dataset[\"text\"].dropna().astype(str)))\n",
    "    plt.figure(figsize=(6, 3))\n",
    "    plt.imshow(wordcloud)\n",
    "    plt.axis('off')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reddit_top = plot_tfidf(reddit_union, 'Top 100 TF-IDF Words for Reddit Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "google_top = plot_tfidf(google_union, 'Top 100 TF-IDF Words for Google Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_top = list(set(reddit_top) & set(google_top))\n",
    "\n",
    "print(\"There are \" + str(len(comumn_top)) + \" common words.\")\n",
    "print(\"They are : \" + str(comumn_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.5 - Word Distribution by Country\n",
    "\n",
    "We also analyzed the distribution of words by country. As shown, the vocabulary does not seem to vary significantly across countries, as the number of common words between two datasets ranges from 7 to 8 out of 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk_top = plot_tfidf(uk_union, 'Top 100 TF-IDF Words for UK Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "au_top = plot_tfidf(au_union, 'Top 100 TF-IDF Words for AU Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_top = plot_tfidf(in_union, 'Top 100 TF-IDF Words for IN Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comumn_uk_au_top = list(set(uk_top) & set(au_top))\n",
    "comumn_au_in_top = list(set(au_top) & set(in_top))\n",
    "comumn_in_uk_top = list(set(in_top) & set(uk_top))\n",
    "\n",
    "print(\"UK and AU\")\n",
    "print(\"There are \" + str(len(comumn_uk_au_top)) + \" common words.\")\n",
    "print(\"They are : \" + str(comumn_uk_au_top))\n",
    "\n",
    "print(\"\\nAU and IN\")\n",
    "print(\"There are \" + str(len(comumn_au_in_top)) + \" common words.\")\n",
    "print(\"They are : \" + str(comumn_au_in_top))\n",
    "\n",
    "print(\"\\nIN and UK\")\n",
    "print(\"There are \" + str(len(comumn_in_uk_top)) + \" common words.\")\n",
    "print(\"They are : \" + str(comumn_in_uk_top))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 - Data Pre-Processing\n",
    "\n",
    "In this step, we remove all characters that are not alphabetical or whitespace. Additionally, we convert the text to lowercase and eliminate consecutive spaces. After cleaning the text, we perform tokenization and lemmazation. Finally, we remove words from the stopwords list, except for those with negation, as they are crucial for our classification task.\n",
    "\n",
    "We initially experimented with stemming as an alternative to lemmatization but ultimately chose the latter. At first, our lemmatization approach did not account for a word’s context, leading to incorrect results. Upon further investigation, we discovered the importance of specifying the `pos` parameter in the \"lemmatize\" function, which determines whether a word is a verb, noun, adjective, or adverb. To ensure accurate classification, we leveraged the `pos_tag` function from `NLTK` to assign the appropriate part of speech before lemmatization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('averaged_perceptron_tagger_eng')\n",
    "nltk.download('wordnet')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "stop_words_remove = {\"no\", \"not\", \"nor\", \"t\"}\n",
    "stop_words.difference_update(stop_words_remove)\n",
    "\n",
    "lemma = nltk.WordNetLemmatizer()\n",
    "token = nltk.word_tokenize\n",
    "\n",
    "def lemmatize_with_pos(text):\n",
    "    words = token(text)   \n",
    "    words_tag = nltk.pos_tag(words)   \n",
    "    words_lem = []\n",
    "    for word, tag in words_tag:\n",
    "        if tag.startswith('N'): words_lem.append(lemma.lemmatize(word, pos='n')) # noun\n",
    "        elif tag.startswith('V'): words_lem.append(lemma.lemmatize(word, pos='v')) # verb\n",
    "        elif tag.startswith('J'): words_lem.append(lemma.lemmatize(word, pos='a')) # adjective\n",
    "        elif tag.startswith('R'): words_lem.append(lemma.lemmatize(word, pos='r')) # adverb\n",
    "        else: words_lem.append(lemma.lemmatize(word))\n",
    "    return words_lem\n",
    "\n",
    "def text_pre_processing(dataset):\n",
    "    # text_vader\n",
    "    dataset['text_vader'] = dataset['text'].apply(contractions.fix)\n",
    "    dataset['text_vader'] = dataset[\"text_vader\"].apply(lambda x: re.sub(r'[^\\x00-\\x7F]|[^a-zA-Z ]', ' ', x).strip()) # remove all the caracteres that do not belong to the alphabet and are not a whitespace\n",
    "    dataset[\"text_vader\"] = dataset[\"text_vader\"].apply(str.lower) # converte all caracteres to lowercase\n",
    "    dataset[\"text_vader\"] = dataset[\"text_vader\"].apply(lambda x: re.sub(r'\\s+', ' ', x).strip()) # remove multiple whitespaces  \n",
    "    # text_processed\n",
    "    dataset['text_processed'] = dataset['text_vader'].apply(token) # apply tokenization    \n",
    "    dataset['text_processed'] = dataset['text_processed'].apply(lambda x: [word for word in x if word not in stop_words]) # remove stopwords, mantaining words like 'no', 'not', 'nor', 't'\n",
    "    # # # apply stemization\n",
    "    # # dataset['text_processed'] = dataset['text_processed'].apply(lambda x: [nltk.PorterStemmer().stem(w) for w in x])    \n",
    "    dataset['text_processed'] = dataset['text_processed'].apply(lambda x: lemmatize_with_pos(\" \".join(x))) # apply lemmazation   \n",
    "    dataset['text_processed'] = [\" \".join(text) for text in dataset[\"text_processed\"]] # join words from the list in a sentence    \n",
    "    return dataset\n",
    "       \n",
    "def metrics(y_test, y_pred, time):\n",
    "    [[tn, fp], [fn, tp]] = (confusion_matrix(y_test, y_pred))    \n",
    "    print(\"                    predicted negative   predicted positive\")\n",
    "    print(\"real negative       \" + str(tn) + (\" \" * (len(str(fn)) - 1)) + \"                \" + str(fp))\n",
    "    print(\"real positive       \" + str(fn) + (\" \" * (len(str(tn)) - 1)) + \"                \" + str(tp))\n",
    "    print(f\"\\nAccuracy {accuracy_score(y_test, y_pred):.2f} Precision {precision_score(y_test, y_pred):.2f} Recall {recall_score(y_test, y_pred):.2f} F1 Score {f1_score(y_test, y_pred):.2f} Time {time:.2f}\") \n",
    "\n",
    "text_pre_processing(global_union)\n",
    "text_pre_processing(uk_union)\n",
    "text_pre_processing(au_union)\n",
    "text_pre_processing(in_union)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making these modifications, we checked for words that were not recognized in `NLTK`'s vocabulary, which likely indicated spelling errors. We attempted to use libraries like `TextBlob` to correct these mistakes but ultimately decided against it due to the high processing time.\n",
    "\n",
    "We then deleted the entries that had an empty `text_processed`  attribute. This could happen due to pre-processing, for example, if the text attribute only consisted of characters that did not belong to the alphabet.\n",
    "\n",
    "After we have made these modifications to the datasets, we save them to our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.download('words')\n",
    "\n",
    "valid_words = set(nltk.corpus.words.words())\n",
    "invalid_words = set()\n",
    "invalid_entries = set()\n",
    "\n",
    "corpus = global_union[\"text_processed\"].dropna().astype(str).tolist()\n",
    "for idx, comment in enumerate(corpus):\n",
    "    words = comment.split()\n",
    "    for word in words:\n",
    "        if word not in valid_words:\n",
    "            invalid_words.add(word)\n",
    "            invalid_entries.add(idx)\n",
    "\n",
    "print(\"There are \" + str(len(invalid_entries)) + \" entries with invalid words\")\n",
    "print(\"The words are \" + str(invalid_words))\n",
    "\n",
    "print(\"\\nRemoval empty entries:\")\n",
    "print(\"- Before we had \" + str(len(global_union)) + \" entries.\")\n",
    "global_union = global_union[global_union[\"text_processed\"].str.len() > 0]\n",
    "print(\"- Now we have \" + str(len(global_union)) + \" entries.\")\n",
    "\n",
    "folder_path = \"data_prepared\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "global_union.to_csv(os.path.join(folder_path, \"global_union.csv\"), index=False, encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Representation Technique\n",
    "Subsequently, we applied a range of feature representation techniques, exploring both sparse and dense vector approaches, as well as `Vader`, that is a sentiment analysis tool.\n",
    "\n",
    "We also analyze the characteristics of our `text_processed` attribute in relation to its length. Notably, the size ranges from a minimum of 1 to a maximum of 770, indicating a significant variation. Additionally, when examining the mean and mode, we observe that most entries have relatively short lengths.\n",
    "\n",
    "This information is particularly relevant for our embedding process, as we employ two different techniques: one that concatenates the embeddings of the first 10 words and another that computes the average embedding of all words in the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = global_union[\"text_processed\"].dropna().astype(str).tolist()\n",
    "lens = [len(c.split()) for c in corpus]\n",
    "print(\"Text Processed's characteristic:\")\n",
    "print(np.min(lens), np.max(lens), np.mean(lens), np.std(lens), stats.mode(lens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Sparse Vectors\n",
    "In relation to sparse vectors:\n",
    "\n",
    "- **CountVectorizer**: the text is converted into a \"bag of words\" (BoW), where the order of the words is not considered. The focus is solely on how many times each word appears in the document, without taking into account the sequence or context in which it occurs.\n",
    "\n",
    "- **TfidfVectorizer**: is similar to CountVectorizer, but with an important difference: it adjusts the word counts by considering not only the frequency of a word in a specific text but also its relative importance in the larger set of documents.\n",
    "\n",
    "- **ngram_range in TfidfVectorizer**: the `ngram_range` parameter in `TfidfVectorizer` allows the model to capture `n-grams`. By setting `ngram_range` to `(1,2)`, the model will consider both unigrams (individual words) and bigrams (pairs of consecutive words), allowing it to capture more contextual information between nearby words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Representation with sparse vectors:\\n\")\n",
    "\n",
    "vectorizer = CountVectorizer(stop_words=None)\n",
    "x_CountVectorizer = vectorizer.fit_transform(corpus)\n",
    "print(\"  - CountVectorizer's shape is \" + str(x_CountVectorizer.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None)\n",
    "x_TfidfVectorizer = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizer's shape is \" + str(x_TfidfVectorizer.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(1,2)) # unigram and bigram\n",
    "x_TfidfVectorizerUB = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizerUB's shape is\" + str(x_TfidfVectorizerUB.shape))\n",
    "\n",
    "vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(2,2)) # bigram\n",
    "x_TfidfVectorizerB = vectorizer.fit_transform(corpus)\n",
    "print(\"  - TfidfVectorizerB's shape is \" + str(x_TfidfVectorizerB.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Dense Vectors\n",
    "Regarding embeddings, we followed two different approaches. In one, we created our own embeddings from our dataset, while in the other, we loaded a pre-trained embeddings file. Additionally, we used different methods to generate sentence embeddings: in one, we concatenated the word vectors, and in the other, we computed their average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Representation with dense vectors:\\n\")\n",
    "\n",
    "def text_to_vector(embeddings, text, sequence_len, mode=\"concat\"):\n",
    "    tokens = text.split()\n",
    "    vec = []\n",
    "    vectors = []\n",
    "    n = 0\n",
    "    i = 0\n",
    "    vector_size = embeddings.vector_size\n",
    "\n",
    "    while i < len(tokens) and n < sequence_len:\n",
    "        try:\n",
    "            word_vector = embeddings.get_vector(tokens[i])\n",
    "            vec.extend(word_vector)\n",
    "            vectors.append(word_vector)\n",
    "            n += 1\n",
    "        except KeyError:\n",
    "            pass\n",
    "        finally: \n",
    "            i += 1\n",
    "\n",
    "    if mode == \"concat\":\n",
    "        vec.extend(np.zeros(vector_size * (sequence_len - n)))\n",
    "        return vec\n",
    "    \n",
    "    elif mode == \"avg\":\n",
    "        if len(vectors) > 0:\n",
    "            return np.mean(vectors, axis=0)\n",
    "        else:\n",
    "            return np.zeros(vector_size)\n",
    "\n",
    "model = Word2Vec([x.split() for x in corpus], vector_size=100, window=3, min_count=2, workers=10, sg=1)\n",
    "word_vectors_corpus = model.wv\n",
    "word_vectors_glovef = KeyedVectors.load_word2vec_format(\"data_prepared/glove.6B.100d.txt\", no_header=True, binary=False)\n",
    "\n",
    "embeddings_corpus_cat = []\n",
    "embeddings_glovef_cat = []\n",
    "embeddings_corpus_avg = []\n",
    "embeddings_glovef_avg = []\n",
    "\n",
    "for c in corpus:\n",
    "    embeddings_corpus_cat.append(text_to_vector(word_vectors_corpus, c, 10))\n",
    "    embeddings_glovef_cat.append(text_to_vector(word_vectors_glovef, c, 10))\n",
    "    embeddings_corpus_avg.append(text_to_vector(word_vectors_corpus, c, 770, \"avg\"))\n",
    "    embeddings_glovef_avg.append(text_to_vector(word_vectors_glovef, c, 770, \"avg\"))\n",
    "\n",
    "x_CorpusCat= np.array(embeddings_corpus_cat)\n",
    "x_CorpusAvg= np.array(embeddings_corpus_avg)\n",
    "print(\"  - CorpusCat's shape is \" + str(x_CorpusCat.shape))\n",
    "print(\"  - CorpusAvg's shape is \" + str(x_CorpusAvg.shape))\n",
    "\n",
    "x_FGloveCat = np.array(embeddings_glovef_cat)\n",
    "x_FGloveAvg = np.array(embeddings_glovef_avg)\n",
    "print(\"  - FGloveCat's shape is \" + str(x_FGloveCat.shape))\n",
    "print(\"  - FGloveAvg's shape is \" + str(x_FGloveAvg.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Vader\n",
    "We also utilize **VADER**, implementing two different approaches. The first, a simpler approach, calculates the polarity of each word and classifies it as positive or negative. The second, a more complex approach, is an algorithm inspired by the professor's slides, which considers additional factors beyond polarity.\n",
    "\n",
    "The simpler version allows us to immediately predict whether a sentiment is positive or negative. It directly calculates the polarity of a text, considering it positive if its polarity is greater than 0 and negative otherwise. However, as observed, the results do not seem very promising.  \n",
    "\n",
    "Due to this, we decided to develop an improved algorithm, that also leverages **VADER**, to achieve better results. To do so, we followed an example algorithm provided by our professor. Instead of calculating the polarity of an entire sentence, this approach counts the number of positive and negative words. Additionally, it considers other factors, such as the presence of words like \"no\" and \"not\", as well as the number of pronouns. The features extracted from this algorithm will later be used for sentiment prediction through classifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sia = SentimentIntensityAnalyzer()\n",
    "y_pred = []\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(global_union['text_processed'], global_union['sentiment_label'], test_size = 0.20, random_state=42)\n",
    "\n",
    "start_time = time.time()\n",
    "for comment in X_test:\n",
    "    y_pred.append(1 if sia.polarity_scores(comment)['compound'] > 0 else 0)\n",
    "end_time = time.time()\n",
    "\n",
    "metrics(y_test, y_pred, end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import StandardScaler\n",
    "\n",
    "\n",
    "x1_list, x2_list, x3_list, x4_list, x5_list, x6_list = [], [], [], [], [], []\n",
    "word_info = dict()\n",
    "pronouns = {\"i\", \"me\", \"my\", \"mine\", \"we\", \"us\", \"our\", \"ours\", \"you\", \"your\", \"yours\"}\n",
    "\n",
    "for line in global_union[\"text_vader\"]:\n",
    "    words = line.split()\n",
    "    x1, x2 = 0, 0\n",
    "    for word in words:\n",
    "        \n",
    "        # if word was already checked\n",
    "        if word in word_info:\n",
    "            if word_info[word] == 1: x1 += 1\n",
    "            elif word_info[word] == -1: x2 += 1\n",
    "            \n",
    "        # if not calculate polarity and add to the dictionary\n",
    "        else:\n",
    "            neg_polarity = sia.polarity_scores(word)[\"neg\"]\n",
    "            pos_polarity = sia.polarity_scores(word)[\"pos\"]\n",
    "            cpd_polarity = sia.polarity_scores(word)[\"compound\"]\n",
    "            if cpd_polarity > 0.05 and pos_polarity > neg_polarity:\n",
    "                word_info[word] = 1\n",
    "                x1 += 1\n",
    "            elif cpd_polarity < -0.05 and pos_polarity < neg_polarity:\n",
    "                word_info[word] = -1\n",
    "                x2 += 1\n",
    "            else:\n",
    "                word_info[word] = 0\n",
    "    \n",
    "    x1_list.append(x1)\n",
    "    x2_list.append(x2)\n",
    "    x3_list.append(1 if \"no\" or \"not\" in words else 0)\n",
    "    x4_list.append(sum(1 for word in words if word in pronouns))\n",
    "    x5_list.append(1 if \"!\" in words else 0)\n",
    "    x6_list.append(math.log(len(words) + 1))\n",
    "\n",
    "df_temp = global_union.copy()\n",
    "df_temp[\"x1\"] = x1_list # number of positive words\n",
    "df_temp[\"x2\"] = x2_list # number of negative words\n",
    "df_temp[\"x3\"] = x3_list # 1 if \"no\" is present and 0 otherwise\n",
    "df_temp[\"x4\"] = x4_list # number of pronouns\n",
    "# df_temp[\"x5\"] = x5_list # 1 if \"!\" is present and 0 otherwise\n",
    "df_temp[\"x6\"] = x6_list # logarithm of text length for normalization.\n",
    "\n",
    "global_union = df_temp\n",
    "    \n",
    "# x_vader = global_union[[\"x1\", \"x2\", \"x3\", \"x4\", \"x5\", \"x6\"]]\n",
    "x_vader = global_union[[\"x1\", \"x2\", \"x3\", \"x4\", \"x6\"]]\n",
    "scaler = StandardScaler()\n",
    "x_vader = scaler.fit_transform(x_vader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    'LogisticRegression': [\n",
    "        {\n",
    "            'penalty': ['l2'],\n",
    "            'solver': ['lbfgs'],\n",
    "            'max_iter': [750, 1125, 1500],\n",
    "        },\n",
    "        {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['liblinear'],\n",
    "            'max_iter': [750, 1125, 1500],\n",
    "        },\n",
    "        {\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'solver': ['saga'],\n",
    "            'max_iter': [750, 1125, 1500],\n",
    "        },\n",
    "        {\n",
    "            'penalty': ['elasticnet'],\n",
    "            'solver': ['saga'],\n",
    "            'max_iter': [750, 1125, 1500],\n",
    "            'l1_ratio': [0.25, 0.5, 0.75],\n",
    "        },\n",
    "    ],\n",
    "    'SGDClassifier': [\n",
    "        {\n",
    "            'loss': ['hinge'],\n",
    "            'penalty': ['l1', 'l2'],\n",
    "            'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "            'eta0': [0.001, 0.01],\n",
    "        },\n",
    "        {\n",
    "            'loss': ['hinge'],\n",
    "            'penalty': ['elasticnet'],\n",
    "            'learning_rate': ['constant', 'optimal', 'invscaling'],\n",
    "            'l1_ratio': [0.25, 0.5, 0.75],\n",
    "            'eta0': [0.001, 0.01],\n",
    "        },\n",
    "\n",
    "        {\n",
    "            'loss': ['log_loss'],\n",
    "            'penalty': ['l2'],\n",
    "            'learning_rate': ['constant', 'invscaling'],\n",
    "            'eta0': [0.001, 0.01],\n",
    "        },\n",
    "        {\n",
    "            'loss': ['log_loss'],\n",
    "            'penalty': ['elasticnet'],\n",
    "            'learning_rate': ['constant', 'invscaling'],\n",
    "            'l1_ratio': [0.25, 0.5, 0.75],\n",
    "            'eta0': [0.001, 0.01],\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "def parameter_tuning(clf, x_predict, y_predict):\n",
    "    grid_search = GridSearchCV(clf, parameters[str(clf.__class__.__name__)], cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(x_predict, y_predict)\n",
    "    return grid_search.best_estimator_, grid_search.best_params_\n",
    "\n",
    "def predict_nb(x, y):\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=42)\n",
    "    clf = MultinomialNB()\n",
    "    # training\n",
    "    start_time = time.time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    y_pred = clf.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "    return X_test, y_test, y_pred\n",
    "\n",
    "def predict_log(x, y):\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.20, random_state=42)\n",
    "    clf = LogisticRegression(tol=1e-2)\n",
    "    # parameter tunning and training \n",
    "    model, param = parameter_tuning(clf, X_train, y_train)\n",
    "    start_time = time.time()   \n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    print(f\"\\nBets parameters: {param}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "    return X_test, y_test, y_pred\n",
    "    \n",
    "def predict_sgd(x, y):\n",
    "    index = global_union.index\n",
    "    # split dataset and define classifier\n",
    "    X_train, X_test, y_train, y_test, index_train, index_test = train_test_split(x, y, index, test_size = 0.20, random_state=42)\n",
    "    clf = SGDClassifier(tol=1e-2)\n",
    "    # parameter tunning and training \n",
    "    model, param = parameter_tuning(clf, X_train, y_train)\n",
    "    start_time = time.time()   \n",
    "    model.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    # testing\n",
    "    print(f\"\\nBets parameters: {param}\")\n",
    "    y_pred = model.predict(X_test)\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "    return X_test, y_test, y_pred, index_test, model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Baseline\n",
    "\n",
    "Initially, we developed a baseline model where the text was not subjected to any preprocessing, and the feature representation was based on a **One Hot Encoder** approach. For classification, we utilized the **Multinomial Naive Bayes** (MultinomialNB) algorithm.\n",
    "\n",
    "Comparing the size of the feature space in our baseline model, which utilizes raw text without any preprocessing, we observe that it is significantly larger than the feature space of the preprocessed text. Specifically, the baseline model results in **29,4548 features**, whereas the preprocessed text yields **19,683 features**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(stop_words=None, lowercase=False, binary=True)\n",
    "x_baseline = vectorizer.fit_transform(global_union['text'].dropna().astype(str).tolist())\n",
    "print(\"Baseline - \" + str(x_baseline.shape))\n",
    "_, _, _ = predict_nb(x_baseline, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Multinomial Naive Bayes\n",
    "\n",
    "Comparing the results obtained with the preprocessed text to those of the baseline model, we observe that the differences in error rates are minimal.\n",
    "\n",
    "We can also observe that, regarding the use of `n-grams`, the model utilizing only `bigrams` yields the poorest results. In contrast, the model that combines `unigrams` and `bigrams` achieves better performance. This approach strikes a good balance, potentially improving the model's ability to handle cases involving negation more effectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse representations\n",
    "print(\"Results for CountVectorizer:\")\n",
    "_, _, _ = predict_nb(x_CountVectorizer, global_union['sentiment_label'])\n",
    "print(\"\\n\\nResults for TfidfVectorizer:\")\n",
    "_, _, _ = predict_nb(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "print(\"\\n\\nResults for TfidfVectorizerUB:\")\n",
    "_, _, _ = predict_nb(x_TfidfVectorizerUB, global_union['sentiment_label'])\n",
    "print(\"\\n\\nResults for TfidfVectorizerB:\")\n",
    "_, _, _ = predict_nb(x_TfidfVectorizerB, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Logistic Regression\n",
    "\n",
    "Based on the results obtained previously, we decided to discontinue testing models with features extracted using **TfidfVectorizer** and **bigrams**. In addition, we explored feature representations using *dense vectors*.\n",
    "\n",
    "In these models, we also perform *hyperparameter tuning*, which increases the execution time of this code block.\n",
    "\n",
    "When comparing Logistic Regression with Multinomial Naive Bayes, we observed that Logistic Regression consistently outperforms Naive Bayes in terms of error rate. However, it requires a longer training time due to its iterative optimization process. In contrast, Naive Bayes is significantly faster, but its performance is generally inferior.  \n",
    "\n",
    "It is evident that the model using features extracted from a Bag of Words approach performs worse than the others utilizing sparse vector features, particularly in terms of precision. Consequently, when testing another classification model later, we will exclude this type of feature from our evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse representations\n",
    "print(\"Results for CountVectorizer:\")\n",
    "_, _, _ = predict_log(x_CountVectorizer, global_union['sentiment_label'])\n",
    "print(\"\\n\\nResults for TfidfVectorizer:\")\n",
    "_, _, _ = predict_log(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "print(\"\\n\\nResults for TfidfVectorizerUB:\")\n",
    "_, _, _ = predict_log(x_TfidfVectorizerUB, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, we found that models trained with sparse vector representations tend to yield better results than those using dense vector representations.\n",
    "\n",
    "The difference in results between concatenating the embeddings of the first 10 words and averaging the embeddings of all words in the text is significant, with the latter yielding superior performance.  \n",
    "\n",
    "A possible explanation for this lies in the varying lengths of texts within the dataset. When using the concatenation method, important content may be lost, making it harder to accurately assess sentiment. Although we experimented with adjusting the number of words concatenated, this did not lead to better results, as shorter texts were disproportionately affected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense representations\n",
    "print(\"Results for CorpusCat:\")\n",
    "_, _, _ = predict_log(x_CorpusCat, global_union['sentiment_label'])\n",
    "print(\"\\n\\nResults for CorpusAvg:\")\n",
    "_, _, _ = predict_log(x_CorpusAvg, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense representations\n",
    "print(\"Results for FGloveCat:\")\n",
    "_, _, _ = predict_log(x_FGloveCat, global_union['sentiment_label'])\n",
    "print(\"\\n\\nResults for FGloveAvg:\")\n",
    "_, _, _ = predict_log(x_FGloveAvg, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm we developed that uses **VADER**, despite obtaining better results than the simple implementation we saw in the previous section, was not very good compared to the others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vader representation\n",
    "print(\"Results for Vader:\")\n",
    "_, _, _ = predict_log(x_vader, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Stochastic Gradient Descent (SGD) Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sparse representations\n",
    "print(\"\\n\\nResults for TfidfVectorizer:\")\n",
    "_, _, _, _, _ = predict_sgd(x_TfidfVectorizer, global_union['sentiment_label'])\n",
    "print(\"\\n\\nResults for TfidfVectorizerUB:\")\n",
    "_, _, _, _, _ = predict_sgd(x_TfidfVectorizerUB, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense representations\n",
    "print(\"\\n\\nResults for CorpusAvg:\")\n",
    "_, _, _, _, _ = predict_sgd(x_CorpusAvg, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dense representations\n",
    "print(\"\\n\\nResults for FGloveAvg:\")\n",
    "_, _, _, _, _ = predict_sgd(x_FGloveAvg, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Error Analysis\n",
    "After the evaluation phase, we selected the model based on **SGDClassifier**, using features extracted by **TfidfVectorizer**. The **`ngram_range=(1,2)`** hyperparameter was set, allowing the model to consider both unigrams (individual words) and bigrams (pairs of consecutive words), resulting in a richer text representation.\n",
    "\n",
    "From the confusion matrix, it is evident that the majority of errors are instances where the model predicts a negative sentiment, whereas the true sentiment is positive.\n",
    "\n",
    "Upon examining the entries that were misclassified, we observed that all of them contain words that are typically associated with a negative sentiment. These words seem to carry a stronger weight in the model’s decision-making process.\n",
    "\n",
    "Based on this observation, we can hypothesize that the model may be placing a disproportionate amount of emphasis on negative words when making sentiment predictions. As a result, even when sentences contain positive words, the model tends to classify them as negative due to the presence of these high-weight negative words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test, y_pred, index_test, model = predict_sgd(x_TfidfVectorizerUB, global_union['sentiment_label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test = np.array(y_test)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "index_invalid = []\n",
    "ypred_invalid = []\n",
    "ytest_invalid = []\n",
    "\n",
    "for i in range(len(index_test)):\n",
    "    if y_test[i] != y_pred[i]:\n",
    "        index_invalid.append(index_test[i])\n",
    "        ypred_invalid.append(y_pred[i])\n",
    "        ytest_invalid.append(y_test[i])\n",
    "    \n",
    "print(index_invalid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(index_invalid)):\n",
    "    entry = global_union.loc[index_invalid[i], [\"text_processed\", \"sentiment_label\"]]\n",
    "    if entry[\"sentiment_label\"] != ytest_invalid[i]:\n",
    "        print(entry)\n",
    "    \n",
    "with open('data_prepared/analysis.txt', 'w') as file:\n",
    "    file.write(f\"Text Real Predicted\\n\")\n",
    "    for i in range(len(index_invalid)):\n",
    "        entry = global_union.loc[index_invalid[i], [\"text\", \"text_processed\", \"sentiment_label\"]]\n",
    "        file.write(f\"{entry[\"text\"]} | {entry['text_processed']} | {ytest_invalid[i]} | {ypred_invalid[i]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sgd_lang(dataset, model):\n",
    "    corpus = dataset[\"text_processed\"].dropna().astype(str).tolist()\n",
    "    vectorizer = TfidfVectorizer(stop_words=None, ngram_range=(1,2)) # unigram and bigram\n",
    "    X = vectorizer.fit_transform(corpus)\n",
    "    y = dataset[\"sentiment_label\"]\n",
    "    \n",
    "    clf = SGDClassifier(**(model.get_params()))\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20, random_state=42)\n",
    "    start_time = time.time()   \n",
    "    clf.fit(X_train, y_train)\n",
    "    y_pred = clf.predict(X_test)\n",
    "    end_time = time.time()\n",
    "    metrics(y_test, y_pred, end_time - start_time)\n",
    "    \n",
    "\n",
    "predict_sgd_lang(uk_union, model)\n",
    "predict_sgd_lang(au_union, model)\n",
    "predict_sgd_lang(in_union, model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
